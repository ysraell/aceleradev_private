{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment E\n",
    "\n",
    "Ideas:\n",
    "\n",
    "1) Implementar formas de processar o dataset\n",
    "\n",
    "1.1) Implementar o baseline com escala de acordo com a variância dos dados, missing values score and entropy.\n",
    "\n",
    "1.2) Implementar o SVD.\n",
    "\n",
    "1.3) Ver como implementar as decomposições em `sklearn.decomposition`.\n",
    "\n",
    "2) Implementar outras distâncias\n",
    "\n",
    "- Author: Israel Oliveira [\\[e-mail\\]](mailto:'Israel%20Oliveira%20'<prof.israel@gmail.com>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NewType, List\n",
    "from loguru import logger\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jax.numpy as npj\n",
    "from jax import jit\n",
    "from time import time\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.decomposition import FactorAnalysis, DictionaryLearning, FastICA, PCA, IncrementalPCA, KernelPCA, LatentDirichletAllocation, NMF, SparsePCA, TruncatedSVD\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-05T00:30:47+00:00\n",
      "\n",
      "CPython 3.7.7\n",
      "IPython 7.15.0\n",
      "\n",
      "compiler   : GCC 8.3.0\n",
      "system     : Linux\n",
      "release    : 4.19.76-linuxkit\n",
      "machine    : x86_64\n",
      "processor  : \n",
      "CPU cores  : 16\n",
      "interpreter: 64bit\n",
      "loguru 0.5.1\n",
      "jax 0.1.71\n",
      "sklearn 0.23.1\n",
      "numpy  1.19.0\n",
      "pandas 1.0.5\n",
      "\n",
      "Git hash: c5395ca1a40f4384557c6639ed899b2b4ee17489\n",
      "Git repo: https://github.com/ysraell/aceleradev_private.git\n",
      "Git branch: master\n"
     ]
    }
   ],
   "source": [
    "# Run this cell before close.\n",
    "%watermark\n",
    "%watermark -p loguru\n",
    "%watermark -p jax\n",
    "%watermark -p sklearn\n",
    "%watermark --iversion\n",
    "%watermark -b -r -g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para fazer o item 1, creio que nesse momento eu preciso de ter alguma forma de ordenar as colunas de acordo com algum critério.\n",
    "a) Pela quantidade de missing values já é um bom começo. Talvez o total de missing dividido pela quantidade de valores únicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-05 00:30:48.065 | INFO     | __main__:<module>:1 - Carregando dataset...\n",
      "2020-07-05 00:30:56.895 | INFO     | __main__:<module>:6 - ...pronto!\n",
      "2020-07-05 00:30:56.896 | INFO     | __main__:<module>:8 - Carregando dataset de validação...\n",
      "2020-07-05 00:30:56.940 | INFO     | __main__:<module>:18 - ...pronto!\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Carregando dataset...\")\n",
    "\n",
    "path_data = '../data/'\n",
    "df_marked = pd.read_csv(path_data+'estaticos_market.csv')\n",
    "df_marked = df_marked.drop(columns=['Unnamed: 0'])\n",
    "logger.info(\"...pronto!\")\n",
    "\n",
    "logger.info(\"Carregando dataset de validação...\")\n",
    "\n",
    "df_ep_list = [pd.read_csv(path_data+'estaticos_portfolio{}.csv'.format(i+1)) for i in range(3)]\n",
    "tmp = []\n",
    "for i in range(3):\n",
    "    df_ep_list[i]['P'] = i+1 \n",
    "    tmp.append(df_ep_list[i][['id','P']])\n",
    "df_ep = pd.concat(tmp)\n",
    "del df_ep_list\n",
    "del tmp\n",
    "logger.info(\"...pronto!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_proc(dataset = df_marked, col_target = 'id', feat_cols = df_marked.columns[2:], N_topcols = -1):\n",
    "    logger.info(\"Processando as features...\")\n",
    "    missing_count = {}\n",
    "    remove_cols = []\n",
    "    for col in feat_cols:\n",
    "        try:\n",
    "            missing_count[col] = sum(dataset[col].isna()) / dataset[col].nunique()\n",
    "            dataset[col] = dataset[col].fillna(0)*1\n",
    "        except ZeroDivisionError:\n",
    "            remove_cols.append(col)\n",
    "\n",
    "    feat_cols = [col for col in feat_cols if col not in remove_cols]\n",
    "\n",
    "    def normalize(x):\n",
    "        return (x-np.min(x))/(np.max(x) - np.min(x)) if (np.max(x) - np.min(x)) > 0 else (x-np.min(x))\n",
    "\n",
    "    for col in feat_cols:\n",
    "        try:\n",
    "            dataset[col] = normalize(dataset[col].tolist())\n",
    "        except:\n",
    "            maping = {val:i+1 for i,val in enumerate(dataset[col].unique())}\n",
    "            dataset[col] = dataset[col].apply(lambda x: maping[x])\n",
    "            dataset[col] = normalize(dataset[col].tolist())\n",
    "\n",
    "    remove_cols = []\n",
    "    for col in feat_cols:\n",
    "        if df_marked[col].nunique() == 1:\n",
    "            remove_cols.append(col)\n",
    "    feat_cols = [col for col in feat_cols if col not in remove_cols]\n",
    "    N_topcols = N_topcols if (N_topcols > 0) and (N_topcols <= len(feat_cols)) else -1\n",
    "    feat_cols_vals = [(col,val) for col,val in list(missing_count.items()) if col in feat_cols]\n",
    "    if N_topcols == -1:\n",
    "        top_cols = feat_cols\n",
    "    else:\n",
    "        top_cols = [col for col,_ in sorted(feat_cols_vals, key=lambda x: x[1])[:N_topcols]]\n",
    "    \n",
    "    missing_count = {key:val for key,val in missing_count.items() if col in feat_cols }\n",
    "    logger.info(\"...pronto!\")\n",
    "    return dataset[[col_target]+top_cols], missing_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funções para cáculo de distâncias e fatorização da matriz do dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Manhattan(X,vec):\n",
    "    return abs(X - vec).sum(-1)\n",
    "\n",
    "def Camberra(X,vec):\n",
    "    return abs((X - vec)/(X + vec)).sum(-1)\n",
    "\n",
    "def BrayCurtis(X,vec):\n",
    "    return abs((X - vec)).sum(-1) / abs((X - vec)).sum(-1).sum(-1)\n",
    "\n",
    "def cossine(X,vec):\n",
    "    O = X*0\n",
    "    for i in range(X.shape[0]):\n",
    "        O[i] = sum(X[i]*vec) / sum(X[i]**2)*sum(vec**2)\n",
    "    return O\n",
    "\n",
    "def cossine_spy(X,vec):\n",
    "    O = X*0\n",
    "    for i in range(X.shape[0]):\n",
    "        O[i] = cosine(X[i],vec)\n",
    "    return O\n",
    "\n",
    "dist_func = [Manhattan, Camberra, BrayCurtis, cossine, cossine_spy]\n",
    "\n",
    "def Nothing(arg):\n",
    "    return arg\n",
    "\n",
    "def npSVD(M):\n",
    "    u, _, _ = np.linalg.svd(M, full_matrices=False)\n",
    "    return u\n",
    "\n",
    "def npSVDj(M):\n",
    "    u, _, _ = npj.linalg.svd(M, full_matrices=False)\n",
    "    return u\n",
    "\n",
    "# Mais rápido!\n",
    "_npSVDj = jit(npSVDj)\n",
    "\n",
    "def _PCA(M,n_components=None):\n",
    "    out = PCA(n_components=n_components)\n",
    "    return out.fit_transform(M)\n",
    "\n",
    "def _FastICA(M,n_components=None):\n",
    "    out = FastICA(n_components=n_components)\n",
    "    return out.fit_transform(M)\n",
    "\n",
    "def _FactorAnalysis(M,n_components=None):\n",
    "    out = FactorAnalysis(n_components=n_components)\n",
    "    return out.fit_transform(M)\n",
    "\n",
    "def _IncrementalPCA(M,n_components=None):\n",
    "    out = IncrementalPCA(n_components=n_components)\n",
    "    return out.fit_transform(M)\n",
    "\n",
    "def _TruncatedSVD(M,n_components=None):\n",
    "    out = TruncatedSVD(n_components=n_components)\n",
    "    return out.fit_transform(M)\n",
    "\n",
    "def _NMF(M,n_components=None):\n",
    "    out = NMF(n_components=n_components)\n",
    "    return out.fit_transform(M)\n",
    "\n",
    "redux_func = [_NMF, _TruncatedSVD, _IncrementalPCA, _FactorAnalysis, _FastICA, _PCA]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funções para processar os valores da matriz do dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def escalaropt_missings(df: pd.DataFrame, score: dict):\n",
    "    df_score = pd.DataFrame(score.items(), columns=['col','score'])\n",
    "    df_score = pd.DataFrame(missing_count.items(), columns=['col','score'])\n",
    "    df_score['escala_opt'] = 1-normalize((np.sqrt(df_score.score)))\n",
    "    #df_score['escala_opt'].sort_values().reset_index(drop=True).plot()\n",
    "    #df_score['escala_opt'].apply(lambda x: max(x,0.1)).sort_values().reset_index(drop=True).plot()\n",
    "    df_score['escala_opt'] = df_score['escala_opt'].apply(lambda x: max(x,0.1))\n",
    "    for row in df_score.iterrows():\n",
    "        df[row.col] = row.escala_opt*df[row.col]\n",
    "    return df\n",
    "\n",
    "def escalaropt_std(df: pd.DataFrame, score: dict):\n",
    "    df_score = pd.DataFrame(score.items(), columns=['col','score'])\n",
    "    df_score = pd.DataFrame(missing_count.items(), columns=['col','score'])\n",
    "    df_score['escala_opt'] = normalize([np.sqrt(np.sqrt(np.sqrt(df[col].std()))) for col in df_score['col']])\n",
    "    #df_score['escala_opt'].sort_values().reset_index(drop=True).plot()\n",
    "    #df_score['escala_opt'].apply(lambda x: max(x,0.1)).sort_values().reset_index(drop=True).plot()\n",
    "    df_score['escala_opt'] = df_score['escala_opt'].apply(lambda x: max(x,0.1))\n",
    "    for row in df_score.iterrows():\n",
    "        df[row.col] = row.escala_opt*df[row.col]\n",
    "    return df\n",
    "\n",
    "def escalaropt_entropy(df: pd.DataFrame, score: dict):\n",
    "    df_score = pd.DataFrame(score.items(), columns=['col','score'])\n",
    "    df_score = pd.DataFrame(missing_count.items(), columns=['col','score'])\n",
    "    df_score['escala_opt'] = normalize([(-sum((df[col]+1)*np.log(df[col]+1))) for col in df_score['col']])\n",
    "    #df_score['escala_opt'].sort_values().reset_index(drop=True).plot()\n",
    "    #df_score['escala_opt'].apply(lambda x: max(x,0.1)).sort_values().reset_index(drop=True).plot()\n",
    "    df_score['escala_opt'] = df_score['escala_opt'].apply(lambda x: max(x,0.1))\n",
    "    for row in df_score.iterrows():\n",
    "        df[row.col] = row.escala_opt*df[row.col]\n",
    "    return df\n",
    "\n",
    "procDS_func = [escalaropt_missings, escalaropt_std, escalaropt_entropy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Uid = NewType('uid', int)\n",
    "Raw = NewType('raw', str)\n",
    "\n",
    "class ExMatrix():\n",
    "    \"\"\"\n",
    "        ************\n",
    "    \"\"\"\n",
    "    def __init__(self,stateless: bool = False):\n",
    "        self.matrix_dict = {}\n",
    "        self.stateless = stateless\n",
    "        self.M = None\n",
    "        self.pu = None\n",
    "        self.raw = None\n",
    "        self.uid = None\n",
    "        self.vector_distance = Manhattan\n",
    "        self.factorize = Nothing\n",
    "        self.process_values = Nothing\n",
    "\n",
    "    def fit(self,dataset: pd.DataFrame):\n",
    "        \"\"\"\n",
    "            ...\n",
    "        \"\"\"\n",
    "        self.raw = dataset[dataset.columns[0]].to_dict()\n",
    "        self.uid = {raw:uid for uid,raw in self.raw.items()}\n",
    "        self.all_raw = dataset[dataset.columns[0]].tolist()\n",
    "        self.all_uid = dataset.index\n",
    "        dataset = self.process_values(dataset)\n",
    "        self.M = self.factorize(dataset[dataset.columns[1:]].values)\n",
    "        del dataset\n",
    "        \n",
    "    def _get_neighbors(self,uid: Uid, k: int = 1, black_list: List[Uid] = []) -> List[Uid]:\n",
    "        \"\"\"\n",
    "            Calcula todas as distâncias entre 'uid' de entrada e todos os outros 'uid'.\n",
    "            A distância calciulada é armazenda e não calculada novamente. \n",
    "        \"\"\"\n",
    "        k = k if k >= 0 else 0\n",
    "        #logger.info(\"Calculando todos os vizinhos...\")\n",
    "        #for uid2 in tqdm(self.trainset.all_users()):\n",
    "        if uid not in self.matrix_dict.keys():\n",
    "            self.matrix_dict[uid] = vector_distance(self.M,self.M[uid])\n",
    "        out = [x[0] for x in sorted(\n",
    "            [\n",
    "                (uid2, self.matrix_dict[uid][uid2])\n",
    "                for uid2 in self.all_uid\n",
    "                if (uid2 not in black_list)\n",
    "            ], key=lambda x: x[1])][:k]\n",
    "        if self.stateless:\n",
    "            del self.matrix_dict\n",
    "            self.matrix_dict = {}\n",
    "        return out\n",
    "    \n",
    "    def _uid2raw(self, uid: Uid)-> str:\n",
    "        '''\n",
    "            uid -> raw.\n",
    "            Valor interno para externo, o nome original do usuário.\n",
    "        '''\n",
    "        return self.raw[uid]\n",
    "    \n",
    "    def _raw2uid(self, raw: Raw)-> int:\n",
    "        '''\n",
    "            raw -> uid.\n",
    "            Valor externo para interno, o id interno do usuários..\n",
    "        '''\n",
    "        return self.uid[raw]\n",
    "    \n",
    "    def recomender(self, in_list: List[Raw], k: int = 1, L: int = 3, Fk: int = 1, limit: int = 100)-> List[Raw]:\n",
    "        '''\n",
    "            Faz as recomendacoes.\n",
    "            ##### Função incompleta #####\n",
    "        '''\n",
    "        # Pega quantas recomendações por usuário em `in_list`,\n",
    "        # mas sem deixar faltar\n",
    "        N_in = len(in_list)\n",
    "        k = k if k > 0 else 1\n",
    "        R_per_in = L*(k//N_in + min(k%N_in,1))\n",
    "\n",
    "        # Pega os `uid`\n",
    "        uid_in_list = [self._raw2uid(raw) for raw in in_list]\n",
    "\n",
    "        # Pega os vizinhos mais próximos de cada uid de entrada.\n",
    "        done = False\n",
    "        flag = True\n",
    "        Rounds = 0\n",
    "        while limit and (not done):\n",
    "            Rounds += 1\n",
    "            # Ele sempre pega todos novamente.\n",
    "            recomendations_list = [self._get_neighbors(uid,R_per_in,uid_in_list) for uid in uid_in_list]\n",
    "            # Quando limit = 0, encerra.\n",
    "            limit -= 1\n",
    "            # Quando tem gente o suficiente, encerra.\n",
    "            if len(set(flat(recomendations_list))) >= Fk*k:\n",
    "                done = True\n",
    "            # Depois do primeiro loop, pega um a mais.\n",
    "            R_per_in += 1\n",
    "\n",
    "        # Aqui gera um dicionário ordenando por votacao.\n",
    "        count_rec = Counter(flat(recomendations_list)) # A votação!!\n",
    "        count_rec = list(count_rec.items())\n",
    "        ct_pos = defaultdict(list)\n",
    "        #ct_pos_inv = defaultdict(list)\n",
    "        while count_rec:\n",
    "            tmp = count_rec.pop(0)\n",
    "            ct_pos[tmp[1]].append(tmp[0])\n",
    "            #ct_pos_inv[tmp[0]].append(tmp[1])\n",
    "\n",
    "        # Aqui considera a posiçao de vizinhos mais proximos.\n",
    "        #nn_pos = defaultdict(list)\n",
    "        nn_pos_inv = defaultdict(list)\n",
    "        tmp = deepcopy(recomendations_list)\n",
    "        while tmp:\n",
    "            tmp2 = tmp.pop(0)\n",
    "            n = 0\n",
    "            while tmp2:\n",
    "                n += 1\n",
    "                tmp3 = tmp2.pop(0)\n",
    "                #nn_pos[n].append(tmp3)\n",
    "                nn_pos_inv[tmp3].append(n)\n",
    "\n",
    "        # Vai separando por votação e ordem de proximidade como desempate.      \n",
    "        votos_list = list(ct_pos.keys())\n",
    "        out_uid = []\n",
    "        while votos_list and k:\n",
    "            votos = max(votos_list)\n",
    "            votos_list.remove(votos)\n",
    "            tmp = sorted([(tmp, min(nn_pos_inv[tmp])) for tmp in ct_pos[votos]], key=lambda x: x[1])\n",
    "            while tmp and k:\n",
    "                out_uid.append(tmp.pop(0)[0])\n",
    "                k -= 1\n",
    "\n",
    "        # converte para Raw e \"joga fora\".\n",
    "        return [self._uid2raw(uid) for uid in out_uid]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-05 00:30:57.024 | INFO     | __main__:feat_proc:2 - Processando as features...\n",
      "2020-07-05 00:31:30.526 | INFO     | __main__:feat_proc:38 - ...pronto!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(462298, 167)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df, missing_count = feat_proc()\n",
    "ex_algo = ExMatrix()\n",
    "ex_algo.fit(df)\n",
    "ex_algo.M.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.516884088516235\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "U1 = npSVD(ex_algo.M)\n",
    "print(time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/jax/lib/xla_bridge.py:125: UserWarning: No GPU/TPU found, falling back to CPU.\n",
      "  warnings.warn('No GPU/TPU found, falling back to CPU.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5774352550506592\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "U2 = _npSVDj(ex_algo.M)\n",
    "print(time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.447381258010864\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "U3 = _PCA(ex_algo.M,n_components=10)\n",
    "print(time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.961236715316772\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "U4 = _FastICA(ex_algo.M,n_components=10)\n",
    "print(time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1553.1734731197357\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "U5 = _FactorAnalysis(ex_algo.M,n_components=10)\n",
    "print(time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.54939031600952\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "U6 = _IncrementalPCA(ex_algo.M)\n",
    "print(time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.877333164215088\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "U7 = _TruncatedSVD(ex_algo.M,n_components=10)\n",
    "print(time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53.063809633255005\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "U8 = _NMF(ex_algo.M,n_components=10)\n",
    "print(time()-t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTAR AS DISTÂNCIAS!!\n",
    "\n",
    "a) Valor correto do cosseno, comparar com as do scipy\n",
    "\n",
    "b) Comparar puro/Jax/Pytran as novas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jit\n",
    "Manhattanj = jit(Manhattan)\n",
    "Camberraj = jit(Camberra)\n",
    "BrayCurtisj = jit(BrayCurtis)\n",
    "dist_funcj = [Manhattanj, Camberraj, BrayCurtisj]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = []\n",
    "times = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \"\"\"\n",
      "/usr/local/lib/python3.7/site-packages/jax/lib/xla_bridge.py:125: UserWarning: No GPU/TPU found, falling back to CPU.\n",
      "  warnings.warn('No GPU/TPU found, falling back to CPU.')\n"
     ]
    }
   ],
   "source": [
    "for _ in range(2):\n",
    "    for dist in dist_func:\n",
    "        t = time()\n",
    "        _ = dist(ex_algo.M,ex_algo.M[0])\n",
    "        times[dist.__name__].append(time()-t)\n",
    "    for dist in dist_funcj:\n",
    "        t = time()\n",
    "        _ = dist(ex_algo.M,ex_algo.M[0])\n",
    "        times[dist.__name__+'j'].append(time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Manhattan</th>\n",
       "      <th>Camberra</th>\n",
       "      <th>BrayCurtis</th>\n",
       "      <th>cossine</th>\n",
       "      <th>cossine_spy</th>\n",
       "      <th>Manhattanj</th>\n",
       "      <th>Camberraj</th>\n",
       "      <th>BrayCurtisj</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.349977</td>\n",
       "      <td>0.958492</td>\n",
       "      <td>0.628118</td>\n",
       "      <td>37.987738</td>\n",
       "      <td>24.223718</td>\n",
       "      <td>1.430388</td>\n",
       "      <td>0.505728</td>\n",
       "      <td>0.551011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.407396</td>\n",
       "      <td>0.570071</td>\n",
       "      <td>0.781777</td>\n",
       "      <td>40.231781</td>\n",
       "      <td>21.598727</td>\n",
       "      <td>0.745282</td>\n",
       "      <td>0.437457</td>\n",
       "      <td>0.489752</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Manhattan  Camberra  BrayCurtis    cossine  cossine_spy  Manhattanj  \\\n",
       "0   0.349977  0.958492    0.628118  37.987738    24.223718    1.430388   \n",
       "1   0.407396  0.570071    0.781777  40.231781    21.598727    0.745282   \n",
       "\n",
       "   Camberraj  BrayCurtisj  \n",
       "0   0.505728     0.551011  \n",
       "1   0.437457     0.489752  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_dict(times, orient='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
