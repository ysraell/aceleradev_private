{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment F\n",
    "\n",
    "2) Implementar um framework de busca de hiperparâmetros.\n",
    "\n",
    "2.1) Parâmetros específicos para cada método de processamento do ds.\n",
    "\n",
    "2.2) N top colunas (`top_cols`) do dataset.\n",
    "\n",
    "2.3) Parâmetro $L$ (`recomender(...,L,...)`).\n",
    "\n",
    "5) Implementar como entrada uma empresa nova, conter mapeamento de valores.\n",
    "\n",
    "Talvez criar o notebook da Second_View, com:\n",
    "1) Verificar `sklearn.inspection.permutation_importance`.\n",
    "\n",
    "- Author: Israel Oliveira [\\[e-mail\\]](mailto:'Israel%20Oliveira%20'<prof.israel@gmail.com>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NewType, List\n",
    "from loguru import logger\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.decomposition import FactorAnalysis, FastICA, PCA, IncrementalPCA, NMF, TruncatedSVD\n",
    "from collections import defaultdict, Counter\n",
    "import functools\n",
    "import operator\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas 1.0.5\n",
      "numpy  1.19.0\n",
      "2020-07-21 \n",
      "\n",
      "CPython 3.7.8\n",
      "IPython 7.16.1\n",
      "\n",
      "compiler   : GCC 8.3.0\n",
      "system     : Linux\n",
      "release    : 4.19.76-linuxkit\n",
      "machine    : x86_64\n",
      "processor  : \n",
      "CPU cores  : 16\n",
      "interpreter: 64bit\n",
      "Git hash   : 00931563c614209b1d6d69a6004408c550d7ffab\n",
      "Git repo   : https://github.com/ysraell/aceleradev_private.git\n",
      "Git branch : master\n"
     ]
    }
   ],
   "source": [
    "# Run this cell before close.\n",
    "%watermark -d --iversion -b -r -g -m -v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-21 01:10:03.559 | INFO     | __main__:<module>:1 - Carregando dataset...\n",
      "2020-07-21 01:10:12.748 | INFO     | __main__:<module>:6 - ...pronto!\n",
      "2020-07-21 01:10:12.749 | INFO     | __main__:<module>:8 - Carregando dataset de validação...\n",
      "2020-07-21 01:10:12.787 | INFO     | __main__:<module>:18 - ...pronto!\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Carregando dataset...\")\n",
    "\n",
    "path_data = '../data/'\n",
    "df_marked = pd.read_csv(path_data+'estaticos_market.csv')\n",
    "df_marked = df_marked.drop(columns=['Unnamed: 0'])\n",
    "logger.info(\"...pronto!\")\n",
    "\n",
    "logger.info(\"Carregando dataset de validação...\")\n",
    "\n",
    "df_ep_list = [pd.read_csv(path_data+'estaticos_portfolio{}.csv'.format(i+1)) for i in range(3)]\n",
    "tmp = []\n",
    "for i in range(3):\n",
    "    df_ep_list[i]['P'] = i+1 \n",
    "    tmp.append(df_ep_list[i][['id','P']])\n",
    "df_ep = pd.concat(tmp)\n",
    "del df_ep_list\n",
    "del tmp\n",
    "logger.info(\"...pronto!\")\n",
    "\n",
    "# Para desenvolvimento do framework:\n",
    "#df_marked = df_marked.merge(df_ep, on='id').drop(columns=['P'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-21 01:10:12.893 | INFO     | __main__:feat_proc:5 - Processando as features...\n",
      "2020-07-21 01:10:47.895 | INFO     | __main__:feat_proc:41 - ...pronto!\n"
     ]
    }
   ],
   "source": [
    "def flat(a):\n",
    "    return functools.reduce(operator.iconcat, a, []) \n",
    "\n",
    "def feat_proc(dataset = df_marked, col_target = 'id', feat_cols = df_marked.columns[1:], N_topcols = -1):\n",
    "    logger.info(\"Processando as features...\")\n",
    "    missing_count = {}\n",
    "    remove_cols = []\n",
    "    for col in feat_cols:\n",
    "        try:\n",
    "            missing_count[col] = sum(dataset[col].isna()) / dataset[col].nunique()\n",
    "            dataset[col] = dataset[col].fillna(0)*1\n",
    "        except ZeroDivisionError:\n",
    "            remove_cols.append(col)\n",
    "\n",
    "    feat_cols = [col for col in feat_cols if col not in remove_cols]\n",
    "\n",
    "    def normalize(x):\n",
    "        return (x-np.min(x))/(np.max(x) - np.min(x)) if (np.max(x) - np.min(x)) > 0 else (x-np.min(x))\n",
    "\n",
    "    for col in feat_cols:\n",
    "        try:\n",
    "            dataset[col] = normalize(dataset[col].tolist())\n",
    "        except:\n",
    "            maping = {val:i+1 for i,val in enumerate(dataset[col].unique())}\n",
    "            dataset[col] = dataset[col].apply(lambda x: maping[x])\n",
    "            dataset[col] = normalize(dataset[col].tolist())\n",
    "\n",
    "    remove_cols = []\n",
    "    for col in feat_cols:\n",
    "        if df_marked[col].nunique() == 1:\n",
    "            remove_cols.append(col)\n",
    "    feat_cols = [col for col in feat_cols if col not in remove_cols]\n",
    "    N_topcols = N_topcols if (N_topcols > 0) and (N_topcols <= len(feat_cols)) else -1\n",
    "    feat_cols_vals = [(col,val) for col,val in list(missing_count.items()) if col in feat_cols]\n",
    "    if N_topcols == -1:\n",
    "        top_cols = feat_cols\n",
    "    else:\n",
    "        top_cols = [col for col,_ in sorted(feat_cols_vals, key=lambda x: x[1])[:N_topcols]]\n",
    "    \n",
    "    missing_count = {key:val for key,val in missing_count.items() if col in feat_cols }\n",
    "    logger.info(\"...pronto!\")\n",
    "    return dataset[[col_target]+top_cols], missing_count\n",
    "\n",
    "\n",
    "\n",
    "def Manhattan(X,vec):\n",
    "    return abs(X - vec).sum(-1)\n",
    "\n",
    "def Camberra(X,vec):\n",
    "    return abs((X - vec)/(X + vec)).sum(-1)\n",
    "\n",
    "def BrayCurtis(X,vec):\n",
    "    return abs((X - vec)).sum(-1) / abs((X - vec)).sum(-1).sum(-1)\n",
    "\n",
    "def np_cossine(X,vec):\n",
    "    return np.array([sum(X[i]*vec) / sum(X[i]**2)*sum(vec**2) for i in range(X.shape[0])])\n",
    "\n",
    "def npj_cossine(X,vec):\n",
    "    return npj.array([sum(X[i]*vec) / sum(X[i]**2)*sum(vec**2) for i in range(X.shape[0])])\n",
    "\n",
    "def scy_cossine(X,vec):\n",
    "    return np.array([cosine(X[i],vec) for i in range(X.shape[0])])\n",
    "\n",
    "#Manhattanj = jit(Manhattan)\n",
    "#Camberraj = jit(Camberra)\n",
    "#BrayCurtisj = jit(BrayCurtis)\n",
    "#np_cossinej = jit(npj_cossine)\n",
    "\n",
    "dist_func = [Manhattan, Camberra, BrayCurtis, np_cossine, scy_cossine]\n",
    "#tmp = [Manhattanj, Camberraj, BrayCurtisj, np_cossinej]\n",
    "\n",
    "#for dist in tmp:\n",
    "#    dist.__name__ += 'j'\n",
    "#\n",
    "#dist_func = dist_func+tmp\n",
    "#del tmp\n",
    "\n",
    "def Nothing(arg):\n",
    "    return arg\n",
    "\n",
    "def npSVD(M):\n",
    "    u, _, _ = np.linalg.svd(M, full_matrices=False)\n",
    "    return u\n",
    "\n",
    "#def npSVDj(M):\n",
    "#    u, _, _ = npj.linalg.svd(M, full_matrices=False)\n",
    "#    return u\n",
    "\n",
    "# Mais rápido!\n",
    "#_npSVDj = jit(npSVDj)\n",
    "\n",
    "\n",
    "def _PCA(M,n_components=None):\n",
    "    out = PCA(n_components=n_components)\n",
    "    return out.fit_transform(M)\n",
    "\n",
    "def _FastICA(M,n_components=None):\n",
    "    out = FastICA(n_components=n_components)\n",
    "    return out.fit_transform(M)\n",
    "\n",
    "def _FactorAnalysis(M,n_components=None):\n",
    "    out = FactorAnalysis(n_components=n_components)\n",
    "    return out.fit_transform(M)\n",
    "\n",
    "def _IncrementalPCA(M,n_components=None):\n",
    "    out = IncrementalPCA(n_components=n_components)\n",
    "    return out.fit_transform(M)\n",
    "\n",
    "def _TruncatedSVD(M,n_components=None):\n",
    "    out = TruncatedSVD(n_components=n_components)\n",
    "    return out.fit_transform(M)\n",
    "\n",
    "def _NMF(M,n_components=None):\n",
    "    out = NMF(n_components=n_components)\n",
    "    return out.fit_transform(M)\n",
    "\n",
    "redux_func = [Nothing, npSVD, _NMF, _TruncatedSVD, _IncrementalPCA, _FactorAnalysis, _FastICA, _PCA]\n",
    "\n",
    "data, missing_count = feat_proc()\n",
    "\n",
    "def escalaropt_missings(df: pd.DataFrame, score: dict):\n",
    "    df_score = pd.DataFrame(score.items(), columns=['col','score'])\n",
    "    df_score = pd.DataFrame(missing_count.items(), columns=['col','score'])\n",
    "    df_score['escala_opt'] = 1-normalize((np.sqrt(df_score.score)))\n",
    "    #df_score['escala_opt'].sort_values().reset_index(drop=True).plot()\n",
    "    #df_score['escala_opt'].apply(lambda x: max(x,0.1)).sort_values().reset_index(drop=True).plot()\n",
    "    df_score['escala_opt'] = df_score['escala_opt'].apply(lambda x: max(x,0.1))\n",
    "    for row in df_score.iterrows():\n",
    "        df[row.col] = row.escala_opt*df[row.col]\n",
    "    return df\n",
    "\n",
    "def escalaropt_std(df: pd.DataFrame, score: dict):\n",
    "    df_score = pd.DataFrame(score.items(), columns=['col','score'])\n",
    "    df_score = pd.DataFrame(missing_count.items(), columns=['col','score'])\n",
    "    df_score['escala_opt'] = normalize([np.sqrt(np.sqrt(np.sqrt(df[col].std()))) for col in df_score['col']])\n",
    "    #df_score['escala_opt'].sort_values().reset_index(drop=True).plot()\n",
    "    #df_score['escala_opt'].apply(lambda x: max(x,0.1)).sort_values().reset_index(drop=True).plot()\n",
    "    df_score['escala_opt'] = df_score['escala_opt'].apply(lambda x: max(x,0.1))\n",
    "    for row in df_score.iterrows():\n",
    "        df[row.col] = row.escala_opt*df[row.col]\n",
    "    return df\n",
    "\n",
    "def escalaropt_entropy(df: pd.DataFrame, score: dict):\n",
    "    df_score = pd.DataFrame(score.items(), columns=['col','score'])\n",
    "    df_score = pd.DataFrame(missing_count.items(), columns=['col','score'])\n",
    "    df_score['escala_opt'] = normalize([(-sum((df[col]+1)*np.log(df[col]+1))) for col in df_score['col']])\n",
    "    #df_score['escala_opt'].sort_values().reset_index(drop=True).plot()\n",
    "    #df_score['escala_opt'].apply(lambda x: max(x,0.1)).sort_values().reset_index(drop=True).plot()\n",
    "    df_score['escala_opt'] = df_score['escala_opt'].apply(lambda x: max(x,0.1))\n",
    "    for row in df_score.iterrows():\n",
    "        df[row.col] = row.escala_opt*df[row.col]\n",
    "    return df\n",
    "\n",
    "procDS_func = [Nothing, escalaropt_missings, escalaropt_std, escalaropt_entropy]\n",
    "\n",
    "Uid = NewType('uid', int)\n",
    "Raw = NewType('raw', str)\n",
    "\n",
    "class ExMatrix():\n",
    "    \"\"\"\n",
    "        ************\n",
    "    \"\"\"\n",
    "    def __init__(self,process_values = Nothing, factorize = Nothing, vector_distance = Manhattan, stateless: bool = False):\n",
    "        self.matrix_dict = {}\n",
    "        self.stateless = stateless\n",
    "        self.M = None\n",
    "        self.pu = None\n",
    "        self.raw = None\n",
    "        self.uid = None\n",
    "        self.vector_distance = vector_distance\n",
    "        self.factorize = factorize\n",
    "        self.process_values = Nothing\n",
    "\n",
    "    def fit(self,dataset: pd.DataFrame):\n",
    "        \"\"\"\n",
    "            ...\n",
    "        \"\"\"\n",
    "        self.raw = dataset[dataset.columns[0]].to_dict()\n",
    "        self.uid = {raw:uid for uid,raw in self.raw.items()}\n",
    "        self.all_raw = dataset[dataset.columns[0]].tolist()\n",
    "        self.all_uid = dataset.index\n",
    "        dataset = self.process_values(dataset)\n",
    "        ds_size = len(dataset[dataset.columns[1:]].values)\n",
    "        self.M = self.factorize(dataset[dataset.columns[1:]].values)\n",
    "        if ds_size != self.M.shape[0]:\n",
    "            raise ValueError('A fatoração não está correta!')\n",
    "        del dataset\n",
    "        \n",
    "    def _get_neighbors(self,uid: Uid, k: int = 1, black_list: List[Uid] = []) -> List[Uid]:\n",
    "        \"\"\"\n",
    "            Calcula todas as distâncias entre 'uid' de entrada e todos os outros 'uid'.\n",
    "            A distância calciulada é armazenda e não calculada novamente. \n",
    "        \"\"\"\n",
    "        k = k if k >= 0 else 0\n",
    "        #logger.info(\"Calculando todos os vizinhos...\")\n",
    "        #for uid2 in tqdm(self.trainset.all_users()):\n",
    "        if uid not in self.matrix_dict.keys():\n",
    "            self.matrix_dict[uid] = self.vector_distance(self.M,self.M[uid])\n",
    "        out = [x[0] for x in sorted(\n",
    "            [\n",
    "                (uid2, self.matrix_dict[uid][uid2])\n",
    "                for uid2 in self.all_uid\n",
    "                if (uid2 not in black_list)\n",
    "            ], key=lambda x: x[1])][:k]\n",
    "        if self.stateless:\n",
    "            del self.matrix_dict\n",
    "            self.matrix_dict = {}\n",
    "        return out\n",
    "    \n",
    "    def _uid2raw(self, uid: Uid)-> str:\n",
    "        '''\n",
    "            uid -> raw.\n",
    "            Valor interno para externo, o nome original do usuário.\n",
    "        '''\n",
    "        return self.raw[uid]\n",
    "    \n",
    "    def _raw2uid(self, raw: Raw)-> int:\n",
    "        '''\n",
    "            raw -> uid.\n",
    "            Valor externo para interno, o id interno do usuários..\n",
    "        '''\n",
    "        return self.uid[raw]\n",
    "    \n",
    "    def recomender(self, in_list: List[Raw], k: int = 1, L: int = 3, Fk: int = 1, limit: int = 10)-> List[Raw]:\n",
    "        '''\n",
    "            Faz as recomendacoes.\n",
    "            ##### Função incompleta #####\n",
    "        '''\n",
    "        # Pega quantas recomendações por usuário em `in_list`,\n",
    "        # mas sem deixar faltar\n",
    "        N_in = len(in_list)\n",
    "        k = k if k > 0 else 1\n",
    "        R_per_in = L*(k//N_in + min(k%N_in,1))\n",
    "\n",
    "        # Pega os `uid`\n",
    "        uid_in_list = [self._raw2uid(raw) for raw in in_list]\n",
    "\n",
    "        # Pega os vizinhos mais próximos de cada uid de entrada.\n",
    "        done = False\n",
    "        flag = True\n",
    "        Rounds = 0\n",
    "        while limit and (not done):\n",
    "            Rounds += 1\n",
    "            # Ele sempre pega todos novamente.\n",
    "            recomendations_list = [self._get_neighbors(uid,R_per_in,uid_in_list) for uid in uid_in_list]\n",
    "            # Quando limit = 0, encerra.\n",
    "            limit -= 1\n",
    "            # Quando tem gente o suficiente, encerra.\n",
    "            if len(set(flat(recomendations_list))) >= Fk*k:\n",
    "                done = True\n",
    "            # Depois do primeiro loop, pega um a mais.\n",
    "            R_per_in += 1\n",
    "\n",
    "        # Aqui gera um dicionário ordenando por votacao.\n",
    "        count_rec = Counter(flat(recomendations_list)) # A votação!!\n",
    "        count_rec = list(count_rec.items())\n",
    "        ct_pos = defaultdict(list)\n",
    "        #ct_pos_inv = defaultdict(list)\n",
    "        while count_rec:\n",
    "            tmp = count_rec.pop(0)\n",
    "            ct_pos[tmp[1]].append(tmp[0])\n",
    "            #ct_pos_inv[tmp[0]].append(tmp[1])\n",
    "\n",
    "        # Aqui considera a posiçao de vizinhos mais proximos.\n",
    "        #nn_pos = defaultdict(list)\n",
    "        nn_pos_inv = defaultdict(list)\n",
    "        tmp = deepcopy(recomendations_list)\n",
    "        while tmp:\n",
    "            tmp2 = tmp.pop(0)\n",
    "            n = 0\n",
    "            while tmp2:\n",
    "                n += 1\n",
    "                tmp3 = tmp2.pop(0)\n",
    "                #nn_pos[n].append(tmp3)\n",
    "                nn_pos_inv[tmp3].append(n)\n",
    "\n",
    "        # Vai separando por votação e ordem de proximidade como desempate.      \n",
    "        votos_list = list(ct_pos.keys())\n",
    "        out_uid = []\n",
    "        while votos_list and k:\n",
    "            votos = max(votos_list)\n",
    "            votos_list.remove(votos)\n",
    "            tmp = sorted([(tmp, min(nn_pos_inv[tmp])) for tmp in ct_pos[votos]], key=lambda x: x[1])\n",
    "            while tmp and k:\n",
    "                out_uid.append(tmp.pop(0)[0])\n",
    "                k -= 1\n",
    "\n",
    "        # converte para Raw e \"joga fora\".\n",
    "        return [self._uid2raw(uid) for uid in out_uid]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Search(N=1, process_values = Nothing, factorize = Nothing, vector_distance_list = [Manhattan]):\n",
    "    ex_algo = ExMatrix(process_values = process_values, factorize = factorize)\n",
    "    ex_algo.fit(data)\n",
    "\n",
    "    out = {}\n",
    "    for dist in vector_distance_list:\n",
    "        ex_algo.vector_distance = dist\n",
    "        print(dist.__name__)\n",
    "        tmp ={1: [0], 2: [0], 3: [0]}\n",
    "        t = time()\n",
    "        for row in tqdm(df_ep.iterrows()):\n",
    "            recs = ex_algo.recomender([row[1].id],k=N)\n",
    "            tmp[row[1].P].append(any([x in df_ep.loc[df_ep.P == row[1].P].id.to_list() for x in recs])*1)\n",
    "        t = time()-t\n",
    "        out[dist.__name__] = {i: (sum(val)/max(1,len(val)), sum(val), len(val)) for i,val in tmp.items()}\n",
    "        out[dist.__name__]['t'] = t\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_list = [BrayCurtis] #[Manhattan, scy_cossine, Camberra, BrayCurtis, np_cossine] #[Manhattan, Camberra, BrayCurtis] #dist_func\n",
    "proc_list = procDS_func #procDS_func #[Nothing] #procDS_func\n",
    "redux_list = [_NMF] #redux_func #[Nothing] #[Nothing, npSVD, _NMF, _PCA, _FactorAnalysis] #redux_func\n",
    "n_components_dict = {Nothing.__name__ : False,\n",
    "                  #_npSVDj.__name__: False,\n",
    "                  npSVD.__name__: False,\n",
    "                  _NMF.__name__ : True,\n",
    "                  _TruncatedSVD.__name__ : True,\n",
    "                  _IncrementalPCA.__name__ : True,\n",
    "                  _FactorAnalysis.__name__ : True,\n",
    "                  _FastICA.__name__ : True,\n",
    "                  _PCA.__name__ : True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_csv_name = 'Results_RC1.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_laoded = pd.read_csv(results_csv_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(results,df_e):\n",
    "    df = pd.DataFrame(results, columns=['pre_proc','redux_func','n_components','dist','t','P1pp','P1_True','P1_len','P2pp','P2_True','P2_len','P3pp','P3_True','P3_len'])\n",
    "    out = pd.concat([df_e, df])\n",
    "    #df_laoded.df_laodedto_csv('Results_redux_prepro.csv',index=False)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Done?: ['Nothing', '_NMF', 30].\n",
      "Done: ['Nothing', '_NMF', 30].\n",
      "Done?: ['Nothing', '_NMF', 32].\n",
      "Done: ['Nothing', '_NMF', 32].\n",
      "Done?: ['Nothing', '_NMF', 34].\n",
      "Done: ['Nothing', '_NMF', 34].\n",
      "Done?: ['Nothing', '_NMF', 36].\n",
      "Done: ['Nothing', '_NMF', 36].\n",
      "Done?: ['Nothing', '_NMF', 38].\n",
      "Done: ['Nothing', '_NMF', 38].\n",
      "Done?: ['Nothing', '_NMF', 40].\n",
      "Done: ['Nothing', '_NMF', 40].\n",
      "Done?: ['Nothing', '_NMF', 42].\n",
      "Done: ['Nothing', '_NMF', 42].\n",
      "Done?: ['Nothing', '_NMF', 44].\n",
      "Done: ['Nothing', '_NMF', 44].\n",
      "Done?: ['Nothing', '_NMF', 46].\n",
      "Done: ['Nothing', '_NMF', 46].\n",
      "Done?: ['Nothing', '_NMF', 48].\n",
      "Done: ['Nothing', '_NMF', 48].\n",
      "Done?: ['Nothing', '_NMF', 50].\n",
      "Done: ['Nothing', '_NMF', 50].\n",
      "Done?: ['Nothing', '_NMF', 52].\n",
      "Done: ['Nothing', '_NMF', 52].\n",
      "Done?: ['Nothing', '_NMF', 54].\n",
      "Done: ['Nothing', '_NMF', 54].\n",
      "Done?: ['Nothing', '_NMF', 56].\n",
      "Done: ['Nothing', '_NMF', 56].\n",
      "Done?: ['Nothing', '_NMF', 58].\n",
      "Done: ['Nothing', '_NMF', 58].\n",
      "Done?: ['Nothing', '_NMF', 60].\n",
      "Done: ['Nothing', '_NMF', 60].\n",
      "Done?: ['Nothing', '_NMF', 62].\n",
      "Done: ['Nothing', '_NMF', 62].\n",
      "Done?: ['Nothing', '_NMF', 64].\n",
      "Done: ['Nothing', '_NMF', 64].\n",
      "Done?: ['Nothing', '_NMF', 66].\n",
      "Done: ['Nothing', '_NMF', 66].\n",
      "Done?: ['Nothing', '_NMF', 68].\n",
      "Done: ['Nothing', '_NMF', 68].\n",
      "Done?: ['Nothing', '_NMF', 70].\n",
      "Done: ['Nothing', '_NMF', 70].\n",
      "Done?: ['Nothing', '_NMF', 72].\n",
      "Done: ['Nothing', '_NMF', 72].\n",
      "Done?: ['Nothing', '_NMF', 74].\n",
      "Done: ['Nothing', '_NMF', 74].\n",
      "Done?: ['Nothing', '_NMF', 76].\n",
      "Done: ['Nothing', '_NMF', 76].\n",
      "Done?: ['Nothing', '_NMF', 78].\n",
      "Done: ['Nothing', '_NMF', 78].\n",
      "Done?: ['Nothing', '_NMF', 80].\n",
      "Done: ['Nothing', '_NMF', 80].\n",
      "Done?: ['Nothing', '_NMF', 85].\n",
      "Done: ['Nothing', '_NMF', 85].\n",
      "Done?: ['Nothing', '_NMF', 90].\n",
      "Done: ['Nothing', '_NMF', 90].\n",
      "Done?: ['Nothing', '_NMF', 95].\n",
      "Done: ['Nothing', '_NMF', 95].\n",
      "Done?: ['Nothing', '_NMF', 100].\n",
      "Done: ['Nothing', '_NMF', 100].\n",
      "Done?: ['Nothing', '_NMF', 105].\n",
      "Done: ['Nothing', '_NMF', 105].\n",
      "Done?: ['Nothing', '_NMF', 110].\n",
      "Done: ['Nothing', '_NMF', 110].\n",
      "Done?: ['Nothing', '_NMF', 115].\n",
      "Done: ['Nothing', '_NMF', 115].\n",
      "Done?: ['Nothing', '_NMF', 120].\n",
      "Done: ['Nothing', '_NMF', 120].\n",
      "Done?: ['Nothing', '_NMF', 125].\n",
      "Done: ['Nothing', '_NMF', 125].\n",
      "Done?: ['Nothing', '_NMF', 130].\n",
      "Done: ['Nothing', '_NMF', 130].\n",
      "Done?: ['Nothing', '_NMF', 135].\n",
      "Done: ['Nothing', '_NMF', 135].\n",
      "Done?: ['Nothing', '_NMF', 140].\n",
      "Done: ['Nothing', '_NMF', 140].\n",
      "Done?: ['Nothing', '_NMF', 145].\n",
      "Done: ['Nothing', '_NMF', 145].\n",
      "2\n",
      "Done?: ['escalaropt_missings', '_NMF', 30].\n",
      "Done: ['escalaropt_missings', '_NMF', 30].\n",
      "Done?: ['escalaropt_missings', '_NMF', 32].\n",
      "Done: ['escalaropt_missings', '_NMF', 32].\n",
      "Done?: ['escalaropt_missings', '_NMF', 34].\n",
      "Done: ['escalaropt_missings', '_NMF', 34].\n",
      "Done?: ['escalaropt_missings', '_NMF', 36].\n",
      "Done: ['escalaropt_missings', '_NMF', 36].\n",
      "Done?: ['escalaropt_missings', '_NMF', 38].\n",
      "Done: ['escalaropt_missings', '_NMF', 38].\n",
      "Done?: ['escalaropt_missings', '_NMF', 40].\n",
      "Done: ['escalaropt_missings', '_NMF', 40].\n",
      "Done?: ['escalaropt_missings', '_NMF', 42].\n",
      "Done: ['escalaropt_missings', '_NMF', 42].\n",
      "Done?: ['escalaropt_missings', '_NMF', 44].\n",
      "Done: ['escalaropt_missings', '_NMF', 44].\n",
      "Done?: ['escalaropt_missings', '_NMF', 46].\n",
      "Done: ['escalaropt_missings', '_NMF', 46].\n",
      "Done?: ['escalaropt_missings', '_NMF', 48].\n",
      "Done: ['escalaropt_missings', '_NMF', 48].\n",
      "Done?: ['escalaropt_missings', '_NMF', 50].\n",
      "Done: ['escalaropt_missings', '_NMF', 50].\n",
      "Done?: ['escalaropt_missings', '_NMF', 52].\n",
      "Done: ['escalaropt_missings', '_NMF', 52].\n",
      "Done?: ['escalaropt_missings', '_NMF', 54].\n",
      "Done: ['escalaropt_missings', '_NMF', 54].\n",
      "Done?: ['escalaropt_missings', '_NMF', 56].\n",
      "Done: ['escalaropt_missings', '_NMF', 56].\n",
      "Done?: ['escalaropt_missings', '_NMF', 58].\n",
      "Done: ['escalaropt_missings', '_NMF', 58].\n",
      "Done?: ['escalaropt_missings', '_NMF', 60].\n",
      "Done: ['escalaropt_missings', '_NMF', 60].\n",
      "Done?: ['escalaropt_missings', '_NMF', 62].\n",
      "Done: ['escalaropt_missings', '_NMF', 62].\n",
      "Done?: ['escalaropt_missings', '_NMF', 64].\n",
      "Done: ['escalaropt_missings', '_NMF', 64].\n",
      "Done?: ['escalaropt_missings', '_NMF', 66].\n",
      "Done: ['escalaropt_missings', '_NMF', 66].\n",
      "Done?: ['escalaropt_missings', '_NMF', 68].\n",
      "Done: ['escalaropt_missings', '_NMF', 68].\n",
      "Done?: ['escalaropt_missings', '_NMF', 70].\n",
      "Done: ['escalaropt_missings', '_NMF', 70].\n",
      "Done?: ['escalaropt_missings', '_NMF', 72].\n",
      "Done: ['escalaropt_missings', '_NMF', 72].\n",
      "Done?: ['escalaropt_missings', '_NMF', 74].\n",
      "Done: ['escalaropt_missings', '_NMF', 74].\n",
      "Done?: ['escalaropt_missings', '_NMF', 76].\n",
      "Done: ['escalaropt_missings', '_NMF', 76].\n",
      "Done?: ['escalaropt_missings', '_NMF', 78].\n",
      "Done: ['escalaropt_missings', '_NMF', 78].\n",
      "Done?: ['escalaropt_missings', '_NMF', 80].\n",
      "Done: ['escalaropt_missings', '_NMF', 80].\n",
      "Done?: ['escalaropt_missings', '_NMF', 85].\n",
      "Done: ['escalaropt_missings', '_NMF', 85].\n",
      "Done?: ['escalaropt_missings', '_NMF', 90].\n",
      "Done: ['escalaropt_missings', '_NMF', 90].\n",
      "Done?: ['escalaropt_missings', '_NMF', 95].\n",
      "Done: ['escalaropt_missings', '_NMF', 95].\n",
      "Done?: ['escalaropt_missings', '_NMF', 100].\n",
      "Done: ['escalaropt_missings', '_NMF', 100].\n",
      "Done?: ['escalaropt_missings', '_NMF', 105].\n",
      "Done: ['escalaropt_missings', '_NMF', 105].\n",
      "Done?: ['escalaropt_missings', '_NMF', 110].\n",
      "Done: ['escalaropt_missings', '_NMF', 110].\n",
      "Done?: ['escalaropt_missings', '_NMF', 115].\n",
      "Done: ['escalaropt_missings', '_NMF', 115].\n",
      "Done?: ['escalaropt_missings', '_NMF', 120].\n",
      "Done: ['escalaropt_missings', '_NMF', 120].\n",
      "Done?: ['escalaropt_missings', '_NMF', 125].\n",
      "Done: ['escalaropt_missings', '_NMF', 125].\n",
      "Done?: ['escalaropt_missings', '_NMF', 130].\n",
      "Done: ['escalaropt_missings', '_NMF', 130].\n",
      "Done?: ['escalaropt_missings', '_NMF', 135].\n",
      "Done: ['escalaropt_missings', '_NMF', 135].\n",
      "Done?: ['escalaropt_missings', '_NMF', 140].\n",
      "Done: ['escalaropt_missings', '_NMF', 140].\n",
      "Done?: ['escalaropt_missings', '_NMF', 145].\n",
      "Done: ['escalaropt_missings', '_NMF', 145].\n",
      "3\n",
      "Done?: ['escalaropt_std', '_NMF', 30].\n",
      "Done: ['escalaropt_std', '_NMF', 30].\n",
      "Done?: ['escalaropt_std', '_NMF', 32].\n",
      "Done: ['escalaropt_std', '_NMF', 32].\n",
      "Done?: ['escalaropt_std', '_NMF', 34].\n",
      "Done: ['escalaropt_std', '_NMF', 34].\n",
      "Done?: ['escalaropt_std', '_NMF', 36].\n",
      "Done: ['escalaropt_std', '_NMF', 36].\n",
      "Done?: ['escalaropt_std', '_NMF', 38].\n",
      "Done: ['escalaropt_std', '_NMF', 38].\n",
      "Done?: ['escalaropt_std', '_NMF', 40].\n",
      "Done: ['escalaropt_std', '_NMF', 40].\n",
      "Done?: ['escalaropt_std', '_NMF', 42].\n",
      "Done: ['escalaropt_std', '_NMF', 42].\n",
      "Done?: ['escalaropt_std', '_NMF', 44].\n",
      "Done: ['escalaropt_std', '_NMF', 44].\n",
      "Done?: ['escalaropt_std', '_NMF', 46].\n",
      "Done: ['escalaropt_std', '_NMF', 46].\n",
      "Done?: ['escalaropt_std', '_NMF', 48].\n",
      "Done: ['escalaropt_std', '_NMF', 48].\n",
      "Done?: ['escalaropt_std', '_NMF', 50].\n",
      "Done: ['escalaropt_std', '_NMF', 50].\n",
      "Done?: ['escalaropt_std', '_NMF', 52].\n",
      "Done: ['escalaropt_std', '_NMF', 52].\n",
      "Done?: ['escalaropt_std', '_NMF', 54].\n",
      "Done: ['escalaropt_std', '_NMF', 54].\n",
      "Done?: ['escalaropt_std', '_NMF', 56].\n",
      "Done: ['escalaropt_std', '_NMF', 56].\n",
      "Done?: ['escalaropt_std', '_NMF', 58].\n",
      "Done: ['escalaropt_std', '_NMF', 58].\n",
      "Done?: ['escalaropt_std', '_NMF', 60].\n",
      "Done: ['escalaropt_std', '_NMF', 60].\n",
      "Done?: ['escalaropt_std', '_NMF', 62].\n",
      "Done: ['escalaropt_std', '_NMF', 62].\n",
      "Done?: ['escalaropt_std', '_NMF', 64].\n",
      "Done: ['escalaropt_std', '_NMF', 64].\n",
      "Done?: ['escalaropt_std', '_NMF', 66].\n",
      "Done: ['escalaropt_std', '_NMF', 66].\n",
      "Done?: ['escalaropt_std', '_NMF', 68].\n",
      "Done: ['escalaropt_std', '_NMF', 68].\n",
      "Done?: ['escalaropt_std', '_NMF', 70].\n",
      "Done: ['escalaropt_std', '_NMF', 70].\n",
      "Done?: ['escalaropt_std', '_NMF', 72].\n",
      "Done: ['escalaropt_std', '_NMF', 72].\n",
      "Done?: ['escalaropt_std', '_NMF', 74].\n",
      "Done: ['escalaropt_std', '_NMF', 74].\n",
      "Done?: ['escalaropt_std', '_NMF', 76].\n",
      "Done: ['escalaropt_std', '_NMF', 76].\n",
      "Done?: ['escalaropt_std', '_NMF', 78].\n",
      "Done: ['escalaropt_std', '_NMF', 78].\n",
      "Done?: ['escalaropt_std', '_NMF', 80].\n",
      "Done: ['escalaropt_std', '_NMF', 80].\n",
      "Done?: ['escalaropt_std', '_NMF', 85].\n",
      "Done: ['escalaropt_std', '_NMF', 85].\n",
      "Done?: ['escalaropt_std', '_NMF', 90].\n",
      "Done: ['escalaropt_std', '_NMF', 90].\n",
      "Done?: ['escalaropt_std', '_NMF', 95].\n",
      "Done: ['escalaropt_std', '_NMF', 95].\n",
      "Done?: ['escalaropt_std', '_NMF', 100].\n",
      "Done: ['escalaropt_std', '_NMF', 100].\n",
      "Done?: ['escalaropt_std', '_NMF', 105].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1077: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  \" improve convergence.\" % max_iter, ConvergenceWarning)\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BrayCurtis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1386it [33:49,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done?: ['escalaropt_std', '_NMF', 110].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1077: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  \" improve convergence.\" % max_iter, ConvergenceWarning)\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BrayCurtis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1386it [33:02,  1.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done?: ['escalaropt_std', '_NMF', 115].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1077: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  \" improve convergence.\" % max_iter, ConvergenceWarning)\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BrayCurtis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1386it [33:48,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done?: ['escalaropt_std', '_NMF', 120].\n",
      "Done: ['escalaropt_std', '_NMF', 120].\n",
      "Done?: ['escalaropt_std', '_NMF', 125].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1077: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  \" improve convergence.\" % max_iter, ConvergenceWarning)\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BrayCurtis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1386it [35:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done?: ['escalaropt_std', '_NMF', 130].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1077: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  \" improve convergence.\" % max_iter, ConvergenceWarning)\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BrayCurtis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1386it [35:09,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done?: ['escalaropt_std', '_NMF', 135].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1077: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  \" improve convergence.\" % max_iter, ConvergenceWarning)\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BrayCurtis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1386it [36:23,  1.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done?: ['escalaropt_std', '_NMF', 140].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1077: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  \" improve convergence.\" % max_iter, ConvergenceWarning)\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BrayCurtis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1386it [36:25,  1.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done?: ['escalaropt_std', '_NMF', 145].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1077: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  \" improve convergence.\" % max_iter, ConvergenceWarning)\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BrayCurtis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1386it [39:49,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Done?: ['escalaropt_entropy', '_NMF', 30].\n",
      "Done: ['escalaropt_entropy', '_NMF', 30].\n",
      "Done?: ['escalaropt_entropy', '_NMF', 32].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1077: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  \" improve convergence.\" % max_iter, ConvergenceWarning)\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BrayCurtis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1386it [22:01,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done?: ['escalaropt_entropy', '_NMF', 34].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1077: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  \" improve convergence.\" % max_iter, ConvergenceWarning)\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BrayCurtis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1386it [24:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done?: ['escalaropt_entropy', '_NMF', 36].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1077: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  \" improve convergence.\" % max_iter, ConvergenceWarning)\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BrayCurtis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1386it [24:35,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done?: ['escalaropt_entropy', '_NMF', 38].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1077: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  \" improve convergence.\" % max_iter, ConvergenceWarning)\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BrayCurtis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1386it [25:39,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done?: ['escalaropt_entropy', '_NMF', 40].\n",
      "Done: ['escalaropt_entropy', '_NMF', 40].\n",
      "Done?: ['escalaropt_entropy', '_NMF', 42].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1077: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  \" improve convergence.\" % max_iter, ConvergenceWarning)\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BrayCurtis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1386it [24:34,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done?: ['escalaropt_entropy', '_NMF', 44].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1077: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  \" improve convergence.\" % max_iter, ConvergenceWarning)\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BrayCurtis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1386it [26:06,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done?: ['escalaropt_entropy', '_NMF', 46].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1077: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  \" improve convergence.\" % max_iter, ConvergenceWarning)\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BrayCurtis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1386it [29:02,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done?: ['escalaropt_entropy', '_NMF', 48].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1077: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  \" improve convergence.\" % max_iter, ConvergenceWarning)\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BrayCurtis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1386it [26:11,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done?: ['escalaropt_entropy', '_NMF', 50].\n",
      "Done: ['escalaropt_entropy', '_NMF', 50].\n",
      "Done?: ['escalaropt_entropy', '_NMF', 52].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1077: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  \" improve convergence.\" % max_iter, ConvergenceWarning)\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BrayCurtis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1386it [26:56,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done?: ['escalaropt_entropy', '_NMF', 54].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1077: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  \" improve convergence.\" % max_iter, ConvergenceWarning)\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BrayCurtis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1386it [27:31,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done?: ['escalaropt_entropy', '_NMF', 56].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1077: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  \" improve convergence.\" % max_iter, ConvergenceWarning)\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BrayCurtis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1386it [27:11,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done?: ['escalaropt_entropy', '_NMF', 58].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1077: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  \" improve convergence.\" % max_iter, ConvergenceWarning)\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BrayCurtis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1386it [27:22,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done?: ['escalaropt_entropy', '_NMF', 60].\n",
      "Done: ['escalaropt_entropy', '_NMF', 60].\n",
      "Done?: ['escalaropt_entropy', '_NMF', 62].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1077: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  \" improve convergence.\" % max_iter, ConvergenceWarning)\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BrayCurtis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1386it [28:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done?: ['escalaropt_entropy', '_NMF', 64].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1077: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  \" improve convergence.\" % max_iter, ConvergenceWarning)\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BrayCurtis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1386it [28:48,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done?: ['escalaropt_entropy', '_NMF', 66].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1077: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  \" improve convergence.\" % max_iter, ConvergenceWarning)\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BrayCurtis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1386it [28:45,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done?: ['escalaropt_entropy', '_NMF', 68].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1077: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  \" improve convergence.\" % max_iter, ConvergenceWarning)\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BrayCurtis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1386it [28:51,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done?: ['escalaropt_entropy', '_NMF', 70].\n",
      "Done: ['escalaropt_entropy', '_NMF', 70].\n",
      "Done?: ['escalaropt_entropy', '_NMF', 72].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1077: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  \" improve convergence.\" % max_iter, ConvergenceWarning)\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BrayCurtis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1386it [28:45,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done?: ['escalaropt_entropy', '_NMF', 74].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1077: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  \" improve convergence.\" % max_iter, ConvergenceWarning)\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BrayCurtis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1386it [31:57,  1.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done?: ['escalaropt_entropy', '_NMF', 76].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1077: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  \" improve convergence.\" % max_iter, ConvergenceWarning)\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BrayCurtis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1386it [30:42,  1.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done?: ['escalaropt_entropy', '_NMF', 78].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1077: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  \" improve convergence.\" % max_iter, ConvergenceWarning)\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BrayCurtis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1386it [28:51,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done?: ['escalaropt_entropy', '_NMF', 80].\n",
      "Done: ['escalaropt_entropy', '_NMF', 80].\n",
      "Done?: ['escalaropt_entropy', '_NMF', 85].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1077: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  \" improve convergence.\" % max_iter, ConvergenceWarning)\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BrayCurtis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1386it [30:45,  1.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done?: ['escalaropt_entropy', '_NMF', 90].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1077: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  \" improve convergence.\" % max_iter, ConvergenceWarning)\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BrayCurtis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1386it [32:20,  1.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done?: ['escalaropt_entropy', '_NMF', 95].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1077: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  \" improve convergence.\" % max_iter, ConvergenceWarning)\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BrayCurtis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1386it [32:15,  1.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done?: ['escalaropt_entropy', '_NMF', 100].\n",
      "Done: ['escalaropt_entropy', '_NMF', 100].\n",
      "Done?: ['escalaropt_entropy', '_NMF', 105].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1077: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  \" improve convergence.\" % max_iter, ConvergenceWarning)\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BrayCurtis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1386it [32:32,  1.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done?: ['escalaropt_entropy', '_NMF', 110].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1077: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  \" improve convergence.\" % max_iter, ConvergenceWarning)\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BrayCurtis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1386it [33:05,  1.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done?: ['escalaropt_entropy', '_NMF', 115].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1077: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  \" improve convergence.\" % max_iter, ConvergenceWarning)\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BrayCurtis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1386it [33:49,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done?: ['escalaropt_entropy', '_NMF', 120].\n",
      "Done: ['escalaropt_entropy', '_NMF', 120].\n",
      "Done?: ['escalaropt_entropy', '_NMF', 125].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1077: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  \" improve convergence.\" % max_iter, ConvergenceWarning)\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BrayCurtis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1386it [34:55,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done?: ['escalaropt_entropy', '_NMF', 130].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1077: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  \" improve convergence.\" % max_iter, ConvergenceWarning)\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BrayCurtis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1386it [35:22,  1.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done?: ['escalaropt_entropy', '_NMF', 135].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1077: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  \" improve convergence.\" % max_iter, ConvergenceWarning)\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BrayCurtis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1386it [36:26,  1.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done?: ['escalaropt_entropy', '_NMF', 140].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1077: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  \" improve convergence.\" % max_iter, ConvergenceWarning)\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BrayCurtis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1386it [36:35,  1.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done?: ['escalaropt_entropy', '_NMF', 145].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1077: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  \" improve convergence.\" % max_iter, ConvergenceWarning)\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BrayCurtis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1386it [36:58,  1.60s/it]\n"
     ]
    }
   ],
   "source": [
    "n_components_list = [n for n in range(30,80,2)]+[n for n in range(80,150,5)]\n",
    "n=0\n",
    "for proc in proc_list:\n",
    "    n += 1\n",
    "    print(n)\n",
    "    for redux in redux_list:\n",
    "        if n_components_dict[redux.__name__]:\n",
    "            for n_components in n_components_list:\n",
    "                cond = (df_laoded['pre_proc'] == proc.__name__) & (df_laoded['redux_func'] == redux.__name__) & (df_laoded['n_components'] == n_components)\n",
    "                print(\"Done?: {}.\".format([proc.__name__, redux.__name__, n_components]))\n",
    "                if sum(cond) == 0:\n",
    "                    def redux_tmp(M):\n",
    "                        return redux(M,n_components=n_components)\n",
    "                    tmp = Search(process_values = proc, factorize = redux_tmp, vector_distance_list = dist_list)\n",
    "                    results = [[proc.__name__, redux.__name__, n_components] + r for r in [[key]+[tmp[key]['t']]+flat([list(tmp[key][i+1]) for i in range(3)]) for key in tmp.keys()]]\n",
    "                    df_laoded = save_results(results,df_laoded)\n",
    "                    df_laoded.to_csv(results_csv_name,index=False)\n",
    "                else:\n",
    "                    print(\"Done: {}.\".format([proc.__name__, redux.__name__, n_components]))\n",
    "        else:\n",
    "            n_components = data.shape[1]\n",
    "            cond = (df_laoded['pre_proc'] == proc.__name__) & (df_laoded['redux_func'] == redux.__name__) & (df_laoded['n_components'] == n_components)\n",
    "            print(\"Done?: {}.\".format([proc.__name__, redux.__name__, n_components]))\n",
    "            if sum(cond) == 0:\n",
    "                tmp = Search(process_values = proc, factorize = redux, vector_distance_list= dist_list)\n",
    "                results = [[proc.__name__, redux.__name__, n_components] + r for r in [[key]+[tmp[key]['t']]+flat([list(tmp[key][i+1]) for i in range(3)]) for key in tmp.keys()]]\n",
    "                df_laoded = save_results(results,df_laoded)\n",
    "                df_laoded.to_csv(results_csv_name,index=False)\n",
    "            else:\n",
    "                print(\"Done: {}.\".format([proc.__name__, redux.__name__, n_components]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.DataFrame(columns=['pre_proc','redux_func','n_components','dist','t','P1pp','P1_True','P1_len','P2pp','P2_True','P2_len','P3pp','P3_True','P3_len'])\n",
    "#df.to_csv(results_csv_name,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_laoded.sort_values(by='t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
