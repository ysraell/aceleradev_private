{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment F\n",
    "\n",
    "2) Implementar um framework de busca de hiperparâmetros.\n",
    "\n",
    "2.1) Parâmetros específicos para cada método de processamento do ds.\n",
    "\n",
    "2.2) N top colunas (`top_cols`) do dataset.\n",
    "\n",
    "2.3) Parâmetro $L$ (`recomender(...,L,...)`).\n",
    "\n",
    "5) Implementar como entrada uma empresa nova, conter mapeamento de valores.\n",
    "\n",
    "Talvez criar o notebook da Second_View, com:\n",
    "1) Verificar `sklearn.inspection.permutation_importance`.\n",
    "\n",
    "- Author: Israel Oliveira [\\[e-mail\\]](mailto:'Israel%20Oliveira%20'<prof.israel@gmail.com>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NewType, List\n",
    "from loguru import logger\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jax.numpy as npj\n",
    "from jax import jit\n",
    "from time import time\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.decomposition import FactorAnalysis, FastICA, PCA, IncrementalPCA, NMF, TruncatedSVD\n",
    "from collections import defaultdict, Counter\n",
    "import functools\n",
    "import operator\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-05T03:18:34+00:00\n",
      "\n",
      "CPython 3.7.7\n",
      "IPython 7.16.1\n",
      "\n",
      "compiler   : GCC 8.3.0\n",
      "system     : Linux\n",
      "release    : 5.4.0-7634-generic\n",
      "machine    : x86_64\n",
      "processor  : \n",
      "CPU cores  : 8\n",
      "interpreter: 64bit\n",
      "loguru 0.5.1\n",
      "jax 0.1.72\n",
      "sklearn 0.23.1\n",
      "numpy  1.19.0\n",
      "pandas 1.0.5\n",
      "\n",
      "Git hash: 020136c9d00459366895fa09dee8bd7680b5d7a9\n",
      "Git repo: https://github.com/ysraell/aceleradev_private.git\n",
      "Git branch: master\n"
     ]
    }
   ],
   "source": [
    "# Run this cell before close.\n",
    "%watermark\n",
    "%watermark -p loguru\n",
    "%watermark -p jax\n",
    "%watermark -p sklearn\n",
    "%watermark --iversion\n",
    "%watermark -b -r -g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-05 03:18:35.299 | INFO     | __main__:<module>:1 - Carregando dataset...\n",
      "2020-07-05 03:18:41.766 | INFO     | __main__:<module>:6 - ...pronto!\n",
      "2020-07-05 03:18:41.767 | INFO     | __main__:<module>:8 - Carregando dataset de validação...\n",
      "2020-07-05 03:18:41.799 | INFO     | __main__:<module>:18 - ...pronto!\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Carregando dataset...\")\n",
    "\n",
    "path_data = '../data/'\n",
    "df_marked = pd.read_csv(path_data+'estaticos_market.csv')\n",
    "df_marked = df_marked.drop(columns=['Unnamed: 0'])\n",
    "logger.info(\"...pronto!\")\n",
    "\n",
    "logger.info(\"Carregando dataset de validação...\")\n",
    "\n",
    "df_ep_list = [pd.read_csv(path_data+'estaticos_portfolio{}.csv'.format(i+1)) for i in range(3)]\n",
    "tmp = []\n",
    "for i in range(3):\n",
    "    df_ep_list[i]['P'] = i+1 \n",
    "    tmp.append(df_ep_list[i][['id','P']])\n",
    "df_ep = pd.concat(tmp)\n",
    "del df_ep_list\n",
    "del tmp\n",
    "logger.info(\"...pronto!\")\n",
    "\n",
    "# Para desenvolvimento do framework:\n",
    "df_marked = df_marked.merge(df_ep, on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-05 03:18:58.236 | INFO     | __main__:feat_proc:5 - Processando as features...\n",
      "2020-07-05 03:18:58.527 | INFO     | __main__:feat_proc:41 - ...pronto!\n"
     ]
    }
   ],
   "source": [
    "def flat(a):\n",
    "    return functools.reduce(operator.iconcat, a, []) \n",
    "\n",
    "def feat_proc(dataset = df_marked, col_target = 'id', feat_cols = df_marked.columns[2:], N_topcols = -1):\n",
    "    logger.info(\"Processando as features...\")\n",
    "    missing_count = {}\n",
    "    remove_cols = []\n",
    "    for col in feat_cols:\n",
    "        try:\n",
    "            missing_count[col] = sum(dataset[col].isna()) / dataset[col].nunique()\n",
    "            dataset[col] = dataset[col].fillna(0)*1\n",
    "        except ZeroDivisionError:\n",
    "            remove_cols.append(col)\n",
    "\n",
    "    feat_cols = [col for col in feat_cols if col not in remove_cols]\n",
    "\n",
    "    def normalize(x):\n",
    "        return (x-np.min(x))/(np.max(x) - np.min(x)) if (np.max(x) - np.min(x)) > 0 else (x-np.min(x))\n",
    "\n",
    "    for col in feat_cols:\n",
    "        try:\n",
    "            dataset[col] = normalize(dataset[col].tolist())\n",
    "        except:\n",
    "            maping = {val:i+1 for i,val in enumerate(dataset[col].unique())}\n",
    "            dataset[col] = dataset[col].apply(lambda x: maping[x])\n",
    "            dataset[col] = normalize(dataset[col].tolist())\n",
    "\n",
    "    remove_cols = []\n",
    "    for col in feat_cols:\n",
    "        if df_marked[col].nunique() == 1:\n",
    "            remove_cols.append(col)\n",
    "    feat_cols = [col for col in feat_cols if col not in remove_cols]\n",
    "    N_topcols = N_topcols if (N_topcols > 0) and (N_topcols <= len(feat_cols)) else -1\n",
    "    feat_cols_vals = [(col,val) for col,val in list(missing_count.items()) if col in feat_cols]\n",
    "    if N_topcols == -1:\n",
    "        top_cols = feat_cols\n",
    "    else:\n",
    "        top_cols = [col for col,_ in sorted(feat_cols_vals, key=lambda x: x[1])[:N_topcols]]\n",
    "    \n",
    "    missing_count = {key:val for key,val in missing_count.items() if col in feat_cols }\n",
    "    logger.info(\"...pronto!\")\n",
    "    return dataset[[col_target]+top_cols], missing_count\n",
    "\n",
    "\n",
    "\n",
    "def Manhattan(X,vec):\n",
    "    return abs(X - vec).sum(-1)\n",
    "\n",
    "def Camberra(X,vec):\n",
    "    return abs((X - vec)/(X + vec)).sum(-1)\n",
    "\n",
    "def BrayCurtis(X,vec):\n",
    "    return abs((X - vec)).sum(-1) / abs((X - vec)).sum(-1).sum(-1)\n",
    "\n",
    "def np_cossine(X,vec):\n",
    "    return np.array([sum(X[i]*vec) / sum(X[i]**2)*sum(vec**2) for i in range(X.shape[0])])\n",
    "\n",
    "def npj_cossine(X,vec):\n",
    "    return npj.array([sum(X[i]*vec) / sum(X[i]**2)*sum(vec**2) for i in range(X.shape[0])])\n",
    "\n",
    "def scy_cossine(X,vec):\n",
    "    return np.array([cosine(X[i],vec) for i in range(X.shape[0])])\n",
    "\n",
    "Manhattanj = jit(Manhattan)\n",
    "Camberraj = jit(Camberra)\n",
    "BrayCurtisj = jit(BrayCurtis)\n",
    "np_cossinej = jit(npj_cossine)\n",
    "\n",
    "dist_func = [Manhattan, Camberra, BrayCurtis, np_cossine, scy_cossine]\n",
    "tmp = [Manhattanj, Camberraj, BrayCurtisj, np_cossinej]\n",
    "\n",
    "for dist in tmp:\n",
    "    dist.__name__ += 'j'\n",
    "\n",
    "dist_func = dist_func+tmp\n",
    "del tmp\n",
    "\n",
    "def Nothing(arg):\n",
    "    return arg\n",
    "\n",
    "def npSVD(M):\n",
    "    u, _, _ = np.linalg.svd(M, full_matrices=False)\n",
    "    return u\n",
    "\n",
    "def npSVDj(M):\n",
    "    u, _, _ = npj.linalg.svd(M, full_matrices=False)\n",
    "    return u\n",
    "\n",
    "# Mais rápido!\n",
    "_npSVDj = jit(npSVDj)\n",
    "\n",
    "\n",
    "def _PCA(M,n_components=None):\n",
    "    out = PCA(n_components=n_components)\n",
    "    return out.fit_transform(M)\n",
    "\n",
    "def _FastICA(M,n_components=None):\n",
    "    out = FastICA(n_components=n_components)\n",
    "    return out.fit_transform(M)\n",
    "\n",
    "def _FactorAnalysis(M,n_components=None):\n",
    "    out = FactorAnalysis(n_components=n_components)\n",
    "    return out.fit_transform(M)\n",
    "\n",
    "def _IncrementalPCA(M,n_components=None):\n",
    "    out = IncrementalPCA(n_components=n_components)\n",
    "    return out.fit_transform(M)\n",
    "\n",
    "def _TruncatedSVD(M,n_components=None):\n",
    "    out = TruncatedSVD(n_components=n_components)\n",
    "    return out.fit_transform(M)\n",
    "\n",
    "def _NMF(M,n_components=None):\n",
    "    out = NMF(n_components=n_components)\n",
    "    return out.fit_transform(M)\n",
    "\n",
    "redux_func = [Nothing, npSVD, _npSVDj, _NMF, _TruncatedSVD, _IncrementalPCA, _FactorAnalysis, _FastICA, _PCA]\n",
    "\n",
    "data, missing_count = feat_proc()\n",
    "\n",
    "def escalaropt_missings(df: pd.DataFrame, score: dict):\n",
    "    df_score = pd.DataFrame(score.items(), columns=['col','score'])\n",
    "    df_score = pd.DataFrame(missing_count.items(), columns=['col','score'])\n",
    "    df_score['escala_opt'] = 1-normalize((np.sqrt(df_score.score)))\n",
    "    #df_score['escala_opt'].sort_values().reset_index(drop=True).plot()\n",
    "    #df_score['escala_opt'].apply(lambda x: max(x,0.1)).sort_values().reset_index(drop=True).plot()\n",
    "    df_score['escala_opt'] = df_score['escala_opt'].apply(lambda x: max(x,0.1))\n",
    "    for row in df_score.iterrows():\n",
    "        df[row.col] = row.escala_opt*df[row.col]\n",
    "    return df\n",
    "\n",
    "def escalaropt_std(df: pd.DataFrame, score: dict):\n",
    "    df_score = pd.DataFrame(score.items(), columns=['col','score'])\n",
    "    df_score = pd.DataFrame(missing_count.items(), columns=['col','score'])\n",
    "    df_score['escala_opt'] = normalize([np.sqrt(np.sqrt(np.sqrt(df[col].std()))) for col in df_score['col']])\n",
    "    #df_score['escala_opt'].sort_values().reset_index(drop=True).plot()\n",
    "    #df_score['escala_opt'].apply(lambda x: max(x,0.1)).sort_values().reset_index(drop=True).plot()\n",
    "    df_score['escala_opt'] = df_score['escala_opt'].apply(lambda x: max(x,0.1))\n",
    "    for row in df_score.iterrows():\n",
    "        df[row.col] = row.escala_opt*df[row.col]\n",
    "    return df\n",
    "\n",
    "def escalaropt_entropy(df: pd.DataFrame, score: dict):\n",
    "    df_score = pd.DataFrame(score.items(), columns=['col','score'])\n",
    "    df_score = pd.DataFrame(missing_count.items(), columns=['col','score'])\n",
    "    df_score['escala_opt'] = normalize([(-sum((df[col]+1)*np.log(df[col]+1))) for col in df_score['col']])\n",
    "    #df_score['escala_opt'].sort_values().reset_index(drop=True).plot()\n",
    "    #df_score['escala_opt'].apply(lambda x: max(x,0.1)).sort_values().reset_index(drop=True).plot()\n",
    "    df_score['escala_opt'] = df_score['escala_opt'].apply(lambda x: max(x,0.1))\n",
    "    for row in df_score.iterrows():\n",
    "        df[row.col] = row.escala_opt*df[row.col]\n",
    "    return df\n",
    "\n",
    "procDS_func = [Nothing, escalaropt_missings, escalaropt_std, escalaropt_entropy]\n",
    "\n",
    "Uid = NewType('uid', int)\n",
    "Raw = NewType('raw', str)\n",
    "\n",
    "class ExMatrix():\n",
    "    \"\"\"\n",
    "        ************\n",
    "    \"\"\"\n",
    "    def __init__(self,process_values = Nothing, factorize = Nothing, vector_distance = Manhattan, stateless: bool = False):\n",
    "        self.matrix_dict = {}\n",
    "        self.stateless = stateless\n",
    "        self.M = None\n",
    "        self.pu = None\n",
    "        self.raw = None\n",
    "        self.uid = None\n",
    "        self.vector_distance = vector_distance\n",
    "        self.factorize = factorize\n",
    "        self.process_values = Nothing\n",
    "\n",
    "    def fit(self,dataset: pd.DataFrame):\n",
    "        \"\"\"\n",
    "            ...\n",
    "        \"\"\"\n",
    "        self.raw = dataset[dataset.columns[0]].to_dict()\n",
    "        self.uid = {raw:uid for uid,raw in self.raw.items()}\n",
    "        self.all_raw = dataset[dataset.columns[0]].tolist()\n",
    "        self.all_uid = dataset.index\n",
    "        dataset = self.process_values(dataset)\n",
    "        ds_size = len(dataset[dataset.columns[1:]].values)\n",
    "        self.M = self.factorize(dataset[dataset.columns[1:]].values)\n",
    "        if ds_size != self.M.shape[0]:\n",
    "            raise ValueError('A fatoração não está correta!')\n",
    "        del dataset\n",
    "        \n",
    "    def _get_neighbors(self,uid: Uid, k: int = 1, black_list: List[Uid] = []) -> List[Uid]:\n",
    "        \"\"\"\n",
    "            Calcula todas as distâncias entre 'uid' de entrada e todos os outros 'uid'.\n",
    "            A distância calciulada é armazenda e não calculada novamente. \n",
    "        \"\"\"\n",
    "        k = k if k >= 0 else 0\n",
    "        #logger.info(\"Calculando todos os vizinhos...\")\n",
    "        #for uid2 in tqdm(self.trainset.all_users()):\n",
    "        if uid not in self.matrix_dict.keys():\n",
    "            self.matrix_dict[uid] = self.vector_distance(self.M,self.M[uid])\n",
    "        out = [x[0] for x in sorted(\n",
    "            [\n",
    "                (uid2, self.matrix_dict[uid][uid2])\n",
    "                for uid2 in self.all_uid\n",
    "                if (uid2 not in black_list)\n",
    "            ], key=lambda x: x[1])][:k]\n",
    "        if self.stateless:\n",
    "            del self.matrix_dict\n",
    "            self.matrix_dict = {}\n",
    "        return out\n",
    "    \n",
    "    def _uid2raw(self, uid: Uid)-> str:\n",
    "        '''\n",
    "            uid -> raw.\n",
    "            Valor interno para externo, o nome original do usuário.\n",
    "        '''\n",
    "        return self.raw[uid]\n",
    "    \n",
    "    def _raw2uid(self, raw: Raw)-> int:\n",
    "        '''\n",
    "            raw -> uid.\n",
    "            Valor externo para interno, o id interno do usuários..\n",
    "        '''\n",
    "        return self.uid[raw]\n",
    "    \n",
    "    def recomender(self, in_list: List[Raw], k: int = 1, L: int = 3, Fk: int = 1, limit: int = 100)-> List[Raw]:\n",
    "        '''\n",
    "            Faz as recomendacoes.\n",
    "            ##### Função incompleta #####\n",
    "        '''\n",
    "        # Pega quantas recomendações por usuário em `in_list`,\n",
    "        # mas sem deixar faltar\n",
    "        N_in = len(in_list)\n",
    "        k = k if k > 0 else 1\n",
    "        R_per_in = L*(k//N_in + min(k%N_in,1))\n",
    "\n",
    "        # Pega os `uid`\n",
    "        uid_in_list = [self._raw2uid(raw) for raw in in_list]\n",
    "\n",
    "        # Pega os vizinhos mais próximos de cada uid de entrada.\n",
    "        done = False\n",
    "        flag = True\n",
    "        Rounds = 0\n",
    "        while limit and (not done):\n",
    "            Rounds += 1\n",
    "            # Ele sempre pega todos novamente.\n",
    "            recomendations_list = [self._get_neighbors(uid,R_per_in,uid_in_list) for uid in uid_in_list]\n",
    "            # Quando limit = 0, encerra.\n",
    "            limit -= 1\n",
    "            # Quando tem gente o suficiente, encerra.\n",
    "            if len(set(flat(recomendations_list))) >= Fk*k:\n",
    "                done = True\n",
    "            # Depois do primeiro loop, pega um a mais.\n",
    "            R_per_in += 1\n",
    "\n",
    "        # Aqui gera um dicionário ordenando por votacao.\n",
    "        count_rec = Counter(flat(recomendations_list)) # A votação!!\n",
    "        count_rec = list(count_rec.items())\n",
    "        ct_pos = defaultdict(list)\n",
    "        #ct_pos_inv = defaultdict(list)\n",
    "        while count_rec:\n",
    "            tmp = count_rec.pop(0)\n",
    "            ct_pos[tmp[1]].append(tmp[0])\n",
    "            #ct_pos_inv[tmp[0]].append(tmp[1])\n",
    "\n",
    "        # Aqui considera a posiçao de vizinhos mais proximos.\n",
    "        #nn_pos = defaultdict(list)\n",
    "        nn_pos_inv = defaultdict(list)\n",
    "        tmp = deepcopy(recomendations_list)\n",
    "        while tmp:\n",
    "            tmp2 = tmp.pop(0)\n",
    "            n = 0\n",
    "            while tmp2:\n",
    "                n += 1\n",
    "                tmp3 = tmp2.pop(0)\n",
    "                #nn_pos[n].append(tmp3)\n",
    "                nn_pos_inv[tmp3].append(n)\n",
    "\n",
    "        # Vai separando por votação e ordem de proximidade como desempate.      \n",
    "        votos_list = list(ct_pos.keys())\n",
    "        out_uid = []\n",
    "        while votos_list and k:\n",
    "            votos = max(votos_list)\n",
    "            votos_list.remove(votos)\n",
    "            tmp = sorted([(tmp, min(nn_pos_inv[tmp])) for tmp in ct_pos[votos]], key=lambda x: x[1])\n",
    "            while tmp and k:\n",
    "                out_uid.append(tmp.pop(0)[0])\n",
    "                k -= 1\n",
    "\n",
    "        # converte para Raw e \"joga fora\".\n",
    "        return [self._uid2raw(uid) for uid in out_uid]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Search(N=1, process_values = Nothing, factorize = Nothing, vector_distance_list = [Manhattan]):\n",
    "    ex_algo = ExMatrix(process_values = process_values, factorize = factorize)\n",
    "    ex_algo.fit(data)\n",
    "\n",
    "    out = {}\n",
    "    for dist in vector_distance_list:\n",
    "        ex_algo.vector_distance = dist\n",
    "        tmp ={1: [], 2: [], 3: []}\n",
    "        for row in tqdm(df_ep.sample(frac=0.01).iterrows()):\n",
    "            recs = ex_algo.recomender([row[1].id],k=N)\n",
    "            tmp[row[1].P].append(any([x in df_ep.loc[df_ep.P == row[1].P].id.to_list() for x in recs])*1)\n",
    "        out[dist.__name__] = {i: (sum(val)/len(val), sum(val), len(val)) for i,val in tmp.items()}\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_list = [Manhattan, Camberraj, BrayCurtisj, cossine_spy]\n",
    "proc_list = procDS_func\n",
    "redux_list = redux_func\n",
    "n_components_dict = {Nothing.__name__ : False,\n",
    "                  _npSVDj.__name__: False,\n",
    "                  _NMF.__name__ : True,\n",
    "                  _TruncatedSVD.__name__ : True,\n",
    "                  _IncrementalPCA.__name__ : True,\n",
    "                  _FactorAnalysis.__name__ : True,\n",
    "                  _FastICA.__name__ : True,\n",
    "                  _PCA.__name__ : True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components_list = [10, 20, 30]\n",
    "\n",
    "results = Search(process_values = Nothing, factorize = Nothing, vector_distance_list= dist_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_algo = ExMatrix(vector_distance=cossine_spy)\n",
    "ex_algo.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in tqdm(df_ep.sample(frac=0.01).iterrows()):\n",
    "    recs = ex_algo.recomender([row[1].id],k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-0eb9df8b440c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mex_algo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_neighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-a28d3c5ea1bf>\u001b[0m in \u001b[0;36m_get_neighbors\u001b[0;34m(self, uid, k, black_list)\u001b[0m\n\u001b[1;32m    205\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0muid2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_uid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0muid2\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mblack_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m             ], key=lambda x: x[1])][:k]\n\u001b[0m\u001b[1;32m    208\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateless\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "ex_algo._get_neighbors(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-d740a6a6bc90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0muid2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_uid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0muid2\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mblack_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     ], key=lambda x: x[1])][:k]\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateless\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "self = ex_algo\n",
    "uid = 1\n",
    "k = 1\n",
    "k = k if k >= 0 else 0\n",
    "black_list = [4, 7]\n",
    "#logger.info(\"Calculando todos os vizinhos...\")\n",
    "#for uid2 in tqdm(self.trainset.all_users()):\n",
    "if uid not in self.matrix_dict.keys():\n",
    "    self.matrix_dict[uid] = self.vector_distance(self.M,self.M[uid])\n",
    "out = [x[0] for x in sorted(\n",
    "    [\n",
    "        (uid2, self.matrix_dict[uid][uid2])\n",
    "        for uid2 in self.all_uid\n",
    "        if (uid2 not in black_list)\n",
    "    ], key=lambda x: x[1])][:k]\n",
    "if self.stateless:\n",
    "    del self.matrix_dict\n",
    "    self.matrix_dict = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=     [\n",
    "        (uid2, self.matrix_dict[uid][uid2])\n",
    "        for uid2 in self.all_uid\n",
    "        if (uid2 not in black_list)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.all_uid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99354571, 0.99354571, 0.99354571, ..., 0.99354571, 0.99354571,\n",
       "        0.99354571],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.38022919, 0.38022919, 0.38022919, ..., 0.38022919, 0.38022919,\n",
       "        0.38022919],\n",
       "       ...,\n",
       "       [0.84666271, 0.84666271, 0.84666271, ..., 0.84666271, 0.84666271,\n",
       "        0.84666271],\n",
       "       [0.39395908, 0.39395908, 0.39395908, ..., 0.39395908, 0.39395908,\n",
       "        0.39395908],\n",
       "       [0.6306781 , 0.6306781 , 0.6306781 , ..., 0.6306781 , 0.6306781 ,\n",
       "        0.6306781 ]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self.vector_distance(self.M,self.M[uid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine(self.M[uid],self.M[uid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = self.M\n",
    "vec = self.M[uid]\n",
    "O = []\n",
    "for i in range(X.shape[0]):\n",
    "    O.append(cosine(X[i],vec))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9935457082169379,\n",
       " 0.0,\n",
       " 0.3802291936279122,\n",
       " 0.8471478546924851,\n",
       " 0.7399987269249657,\n",
       " 0.7971363956704623,\n",
       " 0.5580985522438253,\n",
       " 0.6410108479631822,\n",
       " 0.6552427164976655,\n",
       " 0.7219037191039912,\n",
       " 0.43195685854081245,\n",
       " 0.40244082324946195,\n",
       " 0.8166498268978177,\n",
       " 0.4204418769448268,\n",
       " 0.751687480049672,\n",
       " 0.6069482333574354,\n",
       " 0.44557393212922236,\n",
       " 0.7609261999862933,\n",
       " 0.8304435941910852,\n",
       " 0.8373242660842417,\n",
       " 0.28592201646965854,\n",
       " 0.7717743075937932,\n",
       " 0.7830045502604215,\n",
       " 0.5021061848084716,\n",
       " 0.8031477663709231,\n",
       " 0.791797487811366,\n",
       " 0.3663488744533213,\n",
       " 0.8082107634177241,\n",
       " 0.5545562105693671,\n",
       " 0.8576717530594801,\n",
       " 0.5679536405464326,\n",
       " 0.45359040498126213,\n",
       " 0.5536357577024614,\n",
       " 0.8196570422680466,\n",
       " 0.5545508197101667,\n",
       " 0.7992655230745678,\n",
       " 0.5689660326923076,\n",
       " 0.6151544097290818,\n",
       " 0.43347829114307257,\n",
       " 0.4517642974377467,\n",
       " 0.41804600816454507,\n",
       " 0.7580772165289953,\n",
       " 0.5018617623812456,\n",
       " 0.5216004226095639,\n",
       " 0.8014198856080832,\n",
       " 0.8317332548681919,\n",
       " 0.7821390977773561,\n",
       " 0.7918007133059727,\n",
       " 0.61508322042849,\n",
       " 0.5067338006673494,\n",
       " 0.5203527673064706,\n",
       " 0.34138958562238675,\n",
       " 0.4422627130859196,\n",
       " 0.4850403038706669,\n",
       " 0.36348224832946374,\n",
       " 0.4147208654922302,\n",
       " 0.7908984708775939,\n",
       " 0.4940393776370936,\n",
       " 0.4569953865157327,\n",
       " 0.42965460283943135,\n",
       " 0.642040351980026,\n",
       " 0.6526474333395704,\n",
       " 0.820436566163862,\n",
       " 0.4510082497412017,\n",
       " 0.49560764388368495,\n",
       " 0.6696165564174523,\n",
       " 0.6354249793946305,\n",
       " 0.825351590985145,\n",
       " 0.5464206941045512,\n",
       " 0.6066463944019043,\n",
       " 0.814678174312889,\n",
       " 0.8756988961096976,\n",
       " 0.8780124303048796,\n",
       " 0.5500380909841762,\n",
       " 0.4876962415468321,\n",
       " 0.26390898145700625,\n",
       " 0.7699305831572655,\n",
       " 0.4627870623374669,\n",
       " 0.41684004850875067,\n",
       " 0.8438165572659648,\n",
       " 0.655484351735614,\n",
       " 0.667360644627699,\n",
       " 0.6032306543773398,\n",
       " 0.6175370289451942,\n",
       " 0.7897876315744801,\n",
       " 0.5862721369046359,\n",
       " 0.8241002800716483,\n",
       " 0.47603899169093944,\n",
       " 0.5144580086847761,\n",
       " 0.38215201046786906,\n",
       " 0.6961978321881424,\n",
       " 0.3563077675071319,\n",
       " 0.45654287106196634,\n",
       " 0.7997889656971571,\n",
       " 0.553132970679772,\n",
       " 0.6520404587384041,\n",
       " 0.7028289928993061,\n",
       " 0.7755762224275657,\n",
       " 0.810205933770367,\n",
       " 0.36104207482353856,\n",
       " 0.46919420032272907,\n",
       " 0.5470525631156431,\n",
       " 0.5616414283666616,\n",
       " 0.7834681561672354,\n",
       " 0.8259404976275395,\n",
       " 0.40270521089945066,\n",
       " 0.35743685644590717,\n",
       " 0.6066048972688634,\n",
       " 0.6245437236542064,\n",
       " 0.6862446911308631,\n",
       " 0.4914163146850935,\n",
       " 0.3713057796435273,\n",
       " 0.8624228562497223,\n",
       " 0.523202869686074,\n",
       " 0.6542572405378755,\n",
       " 0.5423066165474051,\n",
       " 0.5545769835845,\n",
       " 0.4838982936948839,\n",
       " 0.5109536397829455,\n",
       " 0.5984929527080749,\n",
       " 0.7978929101853423,\n",
       " 0.43697579001635645,\n",
       " 0.3099762979253987,\n",
       " 0.5453465691005237,\n",
       " 0.8256030555758356,\n",
       " 0.5612352242249914,\n",
       " 0.4882619264568858,\n",
       " 0.8114668732056848,\n",
       " 0.43076165765700625,\n",
       " 0.4625772669992806,\n",
       " 0.6156335625072554,\n",
       " 0.6552377261097793,\n",
       " 0.4220759791760892,\n",
       " 0.8293581944768568,\n",
       " 0.8405842698473431,\n",
       " 0.8277874724629388,\n",
       " 0.6869295261712389,\n",
       " 0.6852505844064012,\n",
       " 0.7687814264475121,\n",
       " 0.8394484505162338,\n",
       " 0.48361754997683637,\n",
       " 0.8053854617903283,\n",
       " 0.4531930120355485,\n",
       " 0.6189942265029127,\n",
       " 0.4178384349829558,\n",
       " 0.4374354089279118,\n",
       " 0.44742259962431574,\n",
       " 0.47555682982331127,\n",
       " 0.2837932711749137,\n",
       " 0.6727588384323013,\n",
       " 0.819229360423721,\n",
       " 0.814727089049014,\n",
       " 0.39825325975863,\n",
       " 0.41222949970639855,\n",
       " 0.43218622625484737,\n",
       " 0.49069500427583657,\n",
       " 0.441380601768948,\n",
       " 0.7794375905282702,\n",
       " 0.37996034554545877,\n",
       " 0.7911937208809932,\n",
       " 0.47736642485923264,\n",
       " 0.6937294892583221,\n",
       " 0.7959380257845655,\n",
       " 0.31386300561260616,\n",
       " 0.8477872677785652,\n",
       " 0.5474906911741395,\n",
       " 0.40031704915664357,\n",
       " 0.58183426458095,\n",
       " 0.5535860771388106,\n",
       " 0.5809860210576557,\n",
       " 0.6141505520270543,\n",
       " 0.2597392902035226,\n",
       " 0.5902402086468302,\n",
       " 0.5785441021332074,\n",
       " 0.5993736497457207,\n",
       " 0.7986226371194552,\n",
       " 0.8326930865709627,\n",
       " 0.7137748673943183,\n",
       " 0.7585208101193694,\n",
       " 0.657992521087146,\n",
       " 0.7888762121879429,\n",
       " 0.4218971868857131,\n",
       " 0.4454906714535951,\n",
       " 0.580942785652312,\n",
       " 0.512260027420334,\n",
       " 0.8068424587858247,\n",
       " 0.6307595202661849,\n",
       " 0.8292504963688287,\n",
       " 0.75237783366062,\n",
       " 0.7630926143705596,\n",
       " 0.3498900449416792,\n",
       " 0.44030614589197214,\n",
       " 0.825853686937842,\n",
       " 0.5107040979750024,\n",
       " 0.5940223645820764,\n",
       " 0.6757102890850941,\n",
       " 0.5938340908662603,\n",
       " 0.44168751733750644,\n",
       " 0.3605975163019709,\n",
       " 0.45764674166520947,\n",
       " 0.6323591959624978,\n",
       " 0.47279885680429423,\n",
       " 0.6490655283066973,\n",
       " 0.4162773875709562,\n",
       " 0.4375624403496796,\n",
       " 0.5541409340342466,\n",
       " 0.8301418154490758,\n",
       " 0.5485251760606693,\n",
       " 0.40136117475428923,\n",
       " 0.5303267131552998,\n",
       " 0.45776171196113613,\n",
       " 0.6039111037141406,\n",
       " 0.8583215880401993,\n",
       " 0.8624997394690586,\n",
       " 0.5257016236736058,\n",
       " 0.5369637924761896,\n",
       " 0.4966414454743402,\n",
       " 0.42837473379261204,\n",
       " 0.6638459786323325,\n",
       " 0.3325869713976173,\n",
       " 0.5785147192849345,\n",
       " 0.7981272409274671,\n",
       " 0.8096663870926328,\n",
       " 0.8123659185204073,\n",
       " 0.4979878062044999,\n",
       " 0.8993788409695388,\n",
       " 0.9009479363415203,\n",
       " 0.807971168352563,\n",
       " 0.8005226552435616,\n",
       " 0.430779911815518,\n",
       " 0.6184772976433737,\n",
       " 0.807437994830303,\n",
       " 0.7975192324298058,\n",
       " 0.7504175413903329,\n",
       " 0.7600895736660376,\n",
       " 0.6483207342351092,\n",
       " 0.38486268321486905,\n",
       " 0.8496366660760424,\n",
       " 0.8407661470395029,\n",
       " 0.43236521558047847,\n",
       " 0.6481800459139475,\n",
       " 0.4701498050723214,\n",
       " 0.5116038094771014,\n",
       " 0.786144977414502,\n",
       " 0.3222263626668844,\n",
       " 0.8590816661806058,\n",
       " 0.5864347694750193,\n",
       " 0.5693241015168794,\n",
       " 0.4459167933195566,\n",
       " 0.8082716480281736,\n",
       " 0.8049380635516205,\n",
       " 0.8145789520363166,\n",
       " 0.618467312845849,\n",
       " 0.38375590261195136,\n",
       " 0.7884512987378332,\n",
       " 0.45147925994642835,\n",
       " 0.386544191112707,\n",
       " 0.6083903272486221,\n",
       " 0.45732232209477497,\n",
       " 0.8341232121589728,\n",
       " 0.8416812787329523,\n",
       " 0.8168324665545996,\n",
       " 0.5861603578364892,\n",
       " 0.3971669618695248,\n",
       " 0.6425614139150087,\n",
       " 0.6970921605351486,\n",
       " 0.811603843250967,\n",
       " 0.3266145619750517,\n",
       " 0.42047775668158227,\n",
       " 0.4279448684060587,\n",
       " 0.43249198048287074,\n",
       " 0.7640937036546654,\n",
       " 0.43707087399018363,\n",
       " 0.44125983541749325,\n",
       " 0.6086430914853367,\n",
       " 0.7787569090869302,\n",
       " 0.8290112368769897,\n",
       " 0.8388551253899307,\n",
       " 0.8245726812398093,\n",
       " 0.4073614229693452,\n",
       " 0.28118684307907216,\n",
       " 0.6531318803815267,\n",
       " 0.5739173922863876,\n",
       " 0.7778269434280793,\n",
       " 0.7879688793439097,\n",
       " 0.7721307669220081,\n",
       " 0.6481924711128354,\n",
       " 0.7825461989510627,\n",
       " 0.7632505888464335,\n",
       " 0.6001318280532013,\n",
       " 0.8495609878789587,\n",
       " 0.4055379714803521,\n",
       " 0.23454829345191386,\n",
       " 0.7319114131664992,\n",
       " 0.43185270563705436,\n",
       " 0.5404577354994389,\n",
       " 0.36039047961483195,\n",
       " 0.5799139672832168,\n",
       " 0.7860276262886556,\n",
       " 0.7952435388211371,\n",
       " 0.3598066808700534,\n",
       " 0.7762082048195271,\n",
       " 0.7844899254618883,\n",
       " 0.8130061363165088,\n",
       " 0.4117660679643149,\n",
       " 0.5619671648922844,\n",
       " 0.4062551634112146,\n",
       " 0.7834369291655336,\n",
       " 0.5031131136043159,\n",
       " 0.6147608730852068,\n",
       " 0.8451150524446536,\n",
       " 0.2648277479667549,\n",
       " 0.7604835991268509,\n",
       " 0.515247164612208,\n",
       " 0.4614184734624198,\n",
       " 0.8866541749216659,\n",
       " 0.5331666595262279,\n",
       " 0.3436993816903554,\n",
       " 0.791148450604694,\n",
       " 0.8265633383899106,\n",
       " 0.7538011822832092,\n",
       " 0.6059797547648722,\n",
       " 0.8195697136007363,\n",
       " 0.834819928820339,\n",
       " 0.44374828086588225,\n",
       " 0.8038774689207672,\n",
       " 0.2606087583687511,\n",
       " 0.3897605338573489,\n",
       " 0.5782483272632435,\n",
       " 0.4673260042399696,\n",
       " 0.7249491884171142,\n",
       " 0.6133108456322522,\n",
       " 0.6336327538423829,\n",
       " 0.8215861143344869,\n",
       " 0.680584922135275,\n",
       " 0.31206798703325533,\n",
       " 0.7912810614615501,\n",
       " 0.40355461739213994,\n",
       " 0.5094356354189715,\n",
       " 0.5872823048281552,\n",
       " 0.5178356458437239,\n",
       " 0.4830889698170179,\n",
       " 0.26874144876885475,\n",
       " 0.5051360230652893,\n",
       " 0.46891630474137935,\n",
       " 0.579368733583524,\n",
       " 0.47772676068422304,\n",
       " 0.38534576794300135,\n",
       " 0.659143362982511,\n",
       " 0.7985640227208628,\n",
       " 0.4026226559470668,\n",
       " 0.425794184753544,\n",
       " 0.4897223647885366,\n",
       " 0.4862299142257791,\n",
       " 0.48275689601073024,\n",
       " 0.49792389382786406,\n",
       " 0.5502001025136629,\n",
       " 0.43411384072589565,\n",
       " 0.6215211643383707,\n",
       " 0.6675216827020836,\n",
       " 0.3035290616923956,\n",
       " 0.3953796161451627,\n",
       " 0.3952423130258488,\n",
       " 0.55551380858992,\n",
       " 0.7398391207526273,\n",
       " 0.31584363678121763,\n",
       " 0.7813466121962448,\n",
       " 0.6581162770675093,\n",
       " 0.4624707775947059,\n",
       " 0.18316444598510573,\n",
       " 0.2821269028848342,\n",
       " 0.39511519488618974,\n",
       " 0.419672739560328,\n",
       " 0.8650147389398681,\n",
       " 0.4079728009998833,\n",
       " 0.8011824206616199,\n",
       " 0.8092914601885269,\n",
       " 0.8187080402474232,\n",
       " 0.8305576177753992,\n",
       " 0.6675231142630568,\n",
       " 0.642405497154636,\n",
       " 0.6191132968175863,\n",
       " 0.5782315459701406,\n",
       " 0.8103189520903824,\n",
       " 0.8353687930600456,\n",
       " 0.39565773561314266,\n",
       " 0.45534245073148183,\n",
       " 0.5224097950645112,\n",
       " 0.7466157767623974,\n",
       " 0.789329832970637,\n",
       " 0.5887465803695986,\n",
       " 0.4785340947036062,\n",
       " 0.4105706510382602,\n",
       " 0.7237343739178033,\n",
       " 0.48652006393726077,\n",
       " 0.31672397397474494,\n",
       " 0.6283154457717287,\n",
       " 0.5897843838356932,\n",
       " 0.7921280864293667,\n",
       " 0.43613301659896087,\n",
       " 0.4779903918811599,\n",
       " 0.3832977026907325,\n",
       " 0.40849290289967266,\n",
       " 0.43261119447965646,\n",
       " 0.6771702097483101,\n",
       " 0.409000307306569,\n",
       " 0.4315702332932959,\n",
       " 0.4347884017974323,\n",
       " 0.7984276373223516,\n",
       " 0.7043763246183125,\n",
       " 0.6325467066709901,\n",
       " 0.5717430807742179,\n",
       " 0.4070679539252031,\n",
       " 0.48833838723608114,\n",
       " 0.8293856245676241,\n",
       " 0.7800114163064227,\n",
       " 0.4254157068185761,\n",
       " 0.8069508256488764,\n",
       " 0.7988967043214337,\n",
       " 0.7708790090637204,\n",
       " 0.7475774603161192,\n",
       " 0.43729133836647516,\n",
       " 0.3814849643092497,\n",
       " 0.40931349247317506,\n",
       " 0.3601886819499375,\n",
       " 0.47003766749542775,\n",
       " 0.3055369334863294,\n",
       " 0.7627750123629615,\n",
       " 0.5024955426988695,\n",
       " 0.48507341513911584,\n",
       " 0.8257760726075498,\n",
       " 0.822515652732344,\n",
       " 0.8441072743932181,\n",
       " 0.8485580632812082,\n",
       " 0.4631923442039655,\n",
       " 0.4783162240692672,\n",
       " 0.754950471041241,\n",
       " 0.4935687665637065,\n",
       " 0.624073848830178,\n",
       " 0.6419401632523396,\n",
       " 0.41243572737549583,\n",
       " 0.8239598164150561,\n",
       " 0.32985677622833576,\n",
       " 0.7965444274920714,\n",
       " 0.4113742706621709,\n",
       " 0.5884302035911237,\n",
       " 0.6967747067457122,\n",
       " 0.41816297045924744,\n",
       " 0.5875647590588091,\n",
       " 0.3955974924692125,\n",
       " 0.5350320506144146,\n",
       " 0.3986096642357203,\n",
       " 0.8282299732512899,\n",
       " 0.4637659689712914,\n",
       " 0.8332242764832593,\n",
       " 0.8432232145165003,\n",
       " 0.8417680857989959,\n",
       " 0.7962964981669413,\n",
       " 0.5131148058451663,\n",
       " 0.5967534940372545,\n",
       " 0.711201890668838,\n",
       " 0.61829090365843,\n",
       " 0.3284123132603036,\n",
       " 0.37896392281618174,\n",
       " 0.8716067704410283,\n",
       " 0.38078402164162306,\n",
       " 0.5811866923136231,\n",
       " 0.588817881901508,\n",
       " 0.6214991160149123,\n",
       " 0.7697708372852816,\n",
       " 0.555463166388048,\n",
       " 0.5114572562875919,\n",
       " 0.8327697836822129,\n",
       " 0.564739987232802,\n",
       " 0.7136988624835342,\n",
       " 0.48571777336454114,\n",
       " 0.49987390591425174,\n",
       " 0.817780675927746,\n",
       " 0.5909806567229924,\n",
       " 0.3671259947032769,\n",
       " 0.5017849408933153,\n",
       " 0.39878058267257765,\n",
       " 0.5590787403464361,\n",
       " 0.31588886633406177,\n",
       " 0.47014277264215476,\n",
       " 0.7374608589483256,\n",
       " 0.7474633253428227,\n",
       " 0.6708842949064272,\n",
       " 0.4053867857423994,\n",
       " 0.5087680304094926,\n",
       " 0.8143021754015123,\n",
       " 0.6421215695459409,\n",
       " 0.5991302424709991,\n",
       " 0.3722702545240488,\n",
       " 0.3933536526513318,\n",
       " 0.5660239478129651,\n",
       " 0.43257411940229185,\n",
       " 0.4192536022574036,\n",
       " 0.49318759326302064,\n",
       " 0.6918538277726185,\n",
       " 0.4097110084108374,\n",
       " 0.43030555931674785,\n",
       " 0.714622074350787,\n",
       " 0.6430982885967477,\n",
       " 0.3313024030036127,\n",
       " 0.33524337988323416,\n",
       " 0.7689847432265524,\n",
       " 0.7938387251755166,\n",
       " 0.7882371356539435,\n",
       " 0.8753316583119847,\n",
       " 0.6507222140008588,\n",
       " 0.5920356288449464,\n",
       " 0.5990202203867139,\n",
       " 0.43305190134801297,\n",
       " 0.6261496598227752,\n",
       " 0.36838079348266595,\n",
       " 0.34092567162313214,\n",
       " 0.7066704922224871,\n",
       " 0.3888145698530122,\n",
       " 0.8254909967913766,\n",
       " 0.8330887059224258,\n",
       " 0.5451225905005366,\n",
       " 0.47199078540764916,\n",
       " 0.4623124608875757,\n",
       " 0.6113533105482702,\n",
       " 0.6645600271617829,\n",
       " 0.5040235990369302,\n",
       " 0.7410766086519995,\n",
       " 0.4007721262103341,\n",
       " 0.8112529749151847,\n",
       " 0.5266609510295038,\n",
       " 0.6427608728051825,\n",
       " 0.7107081231730997,\n",
       " 0.44193464839373386,\n",
       " 0.48867422233597435,\n",
       " 0.3688642647625262,\n",
       " 0.6354474607532561,\n",
       " 0.8177400848372313,\n",
       " 0.7950874183509561,\n",
       " 0.6556193460637386,\n",
       " 0.31096120486503664,\n",
       " 0.6055348730091191,\n",
       " 0.357889594030689,\n",
       " 0.397054056899501,\n",
       " 0.4231776534279035,\n",
       " 0.6474671993885298,\n",
       " 0.7985051939878431,\n",
       " 0.6459633678418762,\n",
       " 0.30254609738903016,\n",
       " 0.6273308811486163,\n",
       " 0.4503316404611373,\n",
       " 0.8224758656479574,\n",
       " 0.6785541082970761,\n",
       " 0.366604875504573,\n",
       " 0.414528302559245,\n",
       " 0.8099218119113613,\n",
       " 0.5246091263643877,\n",
       " 0.7113515847819486,\n",
       " 0.7440459327567842,\n",
       " 0.815827780724117,\n",
       " 0.8235926891048454,\n",
       " 0.8241737289849589,\n",
       " 0.44281354033851783,\n",
       " 0.4274451786956821,\n",
       " 0.41898287035333015,\n",
       " 0.7642704708403187,\n",
       " 0.37829458953034245,\n",
       " 0.4158000316670035,\n",
       " 0.7361682518713761,\n",
       " 0.426421327536218,\n",
       " 0.46998628429906275,\n",
       " 0.6409880513794318,\n",
       " 0.8374963071880295,\n",
       " 0.6973560447271742,\n",
       " 0.3717980757407586,\n",
       " 0.6176976814169146,\n",
       " 0.8646493268429548,\n",
       " 0.7756445500858721,\n",
       " 0.6201008034471942,\n",
       " 0.3385477975729497,\n",
       " 0.3982328923244862,\n",
       " 0.4717872855596249,\n",
       " 0.8632984283626545,\n",
       " 0.6350933898811757,\n",
       " 0.606516959143799,\n",
       " 0.3929471649056969,\n",
       " 0.7728295709501125,\n",
       " 0.8370877141989345,\n",
       " 0.5261446445319317,\n",
       " 0.7278783501450241,\n",
       " 0.5843758013609704,\n",
       " 0.5873466149861308,\n",
       " 0.5490365574551452,\n",
       " 0.5637901584668559,\n",
       " 0.4525402978242128,\n",
       " 0.4950788545348549,\n",
       " 0.3673237474122326,\n",
       " 0.8581912840811977,\n",
       " 0.45015591586310344,\n",
       " 0.7743115855857036,\n",
       " 0.7862290969288359,\n",
       " 0.4457387718908561,\n",
       " 0.4585858663414131,\n",
       " 0.591801836110241,\n",
       " 0.7400877238173422,\n",
       " 0.49804977394164374,\n",
       " 0.6313451368877426,\n",
       " 0.8003399038454557,\n",
       " 0.8107166344963079,\n",
       " 0.6387052399869915,\n",
       " 0.8024823543621694,\n",
       " 0.5296080422911112,\n",
       " 0.6643491840404518,\n",
       " 0.7673105591481751,\n",
       " 0.3766783882543895,\n",
       " 0.6899887167881934,\n",
       " 0.7982993926489167,\n",
       " 0.39564654810106237,\n",
       " 0.7046560622742435,\n",
       " 0.21625953194665648,\n",
       " 0.6389141440263031,\n",
       " 0.8195697344378569,\n",
       " 0.829642276193592,\n",
       " 0.6094917857926792,\n",
       " 0.46941281747938846,\n",
       " 0.6475892237068184,\n",
       " 0.4607088840166528,\n",
       " 0.38856021745472435,\n",
       " 0.810927102419087,\n",
       " 0.3682288412598309,\n",
       " 0.7217363758643108,\n",
       " 0.4813815567806323,\n",
       " 0.38597232573198403,\n",
       " 0.47406106417277927,\n",
       " 0.304352892836915,\n",
       " 0.32288503886657693,\n",
       " 0.5852731320438355,\n",
       " 0.4560745554227874,\n",
       " 0.45685414393038914,\n",
       " 0.5528793994916001,\n",
       " 0.8464038580633197,\n",
       " 0.3891338113061602,\n",
       " 0.8021081505897161,\n",
       " 0.4195092736795114,\n",
       " 0.4014396516959603,\n",
       " 0.4020271343973376,\n",
       " 0.42500932598142704,\n",
       " 0.6680763469372752,\n",
       " 0.5826725571176524,\n",
       " 0.8332273848681492,\n",
       " 0.6694597282645876,\n",
       " 0.7753295352260186,\n",
       " 0.7203277908976558,\n",
       " 0.6622722636309791,\n",
       " 0.4438808080283192,\n",
       " 0.7857755656812877,\n",
       " 0.6220072458194437,\n",
       " 0.44586995666980656,\n",
       " 0.405300628454086,\n",
       " 0.48363961270670974,\n",
       " 0.7926574935953168,\n",
       " 0.8003192664817751,\n",
       " 0.7364794261263363,\n",
       " 0.7509077203287381,\n",
       " 0.17025452551518294,\n",
       " 0.8116850199898931,\n",
       " 0.4007807547374185,\n",
       " 0.8287938489233849,\n",
       " 0.7977754007305615,\n",
       " 0.37549696244297215,\n",
       " 0.4868207432671684,\n",
       " 0.3579201664350339,\n",
       " 0.494251429055542,\n",
       " 0.6806019108784902,\n",
       " 0.6594498074627104,\n",
       " 0.7832325739098657,\n",
       " 0.6459228483102136,\n",
       " 0.4486042785221239,\n",
       " 0.798858346509849,\n",
       " 0.56021451138705,\n",
       " 0.34706047957372554,\n",
       " 0.35635616469196263,\n",
       " 0.6332463786116104,\n",
       " 0.8534881653866824,\n",
       " 0.47792977924253655,\n",
       " 0.5580130938459256,\n",
       " 0.8200233089485512,\n",
       " 0.44832272550336094,\n",
       " 0.425863163811526,\n",
       " 0.38968419552059363,\n",
       " 0.6233773021001388,\n",
       " 0.5353815642267286,\n",
       " 0.44724815830714615,\n",
       " 0.6373113205105606,\n",
       " 0.5440264672127448,\n",
       " 0.383596131332421,\n",
       " 0.45865572377836306,\n",
       " 0.4755756808757384,\n",
       " 0.8285038412325079,\n",
       " 0.8362577787482325,\n",
       " 0.8472878306806795,\n",
       " 0.5040071175334524,\n",
       " 0.29177348145495907,\n",
       " 0.3917474261439641,\n",
       " 0.8062270549558136,\n",
       " 0.8144886798276886,\n",
       " 0.6051026122504255,\n",
       " 0.7976948547822043,\n",
       " 0.527799973875328,\n",
       " 0.826021051693221,\n",
       " 0.8211741440794735,\n",
       " 0.32466213516857734,\n",
       " 0.46011817818739,\n",
       " 0.7809595753053654,\n",
       " 0.49564111686533674,\n",
       " 0.6196165289655374,\n",
       " 0.824036768723749,\n",
       " 0.8336069754732267,\n",
       " 0.6421725140144636,\n",
       " 0.754688069508951,\n",
       " 0.5561973107752465,\n",
       " 0.33036974379718675,\n",
       " 0.6818167801085229,\n",
       " 0.4524688875259514,\n",
       " 0.3971868254015163,\n",
       " 0.8267356272588374,\n",
       " 0.614864328982431,\n",
       " 0.5821636084020269,\n",
       " 0.4149107190775465,\n",
       " 0.832853357469762,\n",
       " 0.8306593382271306,\n",
       " 0.36515508020716425,\n",
       " 0.43609984768210286,\n",
       " 0.6339790094446689,\n",
       " 0.35112338296321843,\n",
       " 0.5547421696943126,\n",
       " 0.4283279991159362,\n",
       " 0.6311732233290847,\n",
       " 0.6093777190984724,\n",
       " 0.6257518799165545,\n",
       " 0.43309122430322555,\n",
       " 0.8017154517360453,\n",
       " 0.8286408140929187,\n",
       " 0.49116444052227437,\n",
       " 0.471008535095382,\n",
       " 0.3976341653220923,\n",
       " 0.37340313797686175,\n",
       " 0.4681430689090944,\n",
       " 0.5715427579531208,\n",
       " 0.5871637285194756,\n",
       " 0.621270244360216,\n",
       " 0.6077108235294499,\n",
       " 0.44707776596998183,\n",
       " 0.6339758901780304,\n",
       " 0.7843363978791756,\n",
       " 0.6185098053777299,\n",
       " 0.4099920843466943,\n",
       " 0.6537626954265812,\n",
       " 0.7979316166063724,\n",
       " 0.6600805414912638,\n",
       " 0.4005662665159012,\n",
       " 0.5519854541103763,\n",
       " 0.8245699533738337,\n",
       " 0.47950154018921876,\n",
       " 0.7960949111990365,\n",
       " 0.4006610583994006,\n",
       " 0.8394168004559075,\n",
       " 0.38114063405818777,\n",
       " 0.26188507194680877,\n",
       " 0.6018491652989577,\n",
       " 0.4323497400581875,\n",
       " 0.4037601131099612,\n",
       " 0.6090876478051086,\n",
       " 0.8062372728179299,\n",
       " 0.5994474481160903,\n",
       " 0.4224183560308711,\n",
       " 0.4389371442056905,\n",
       " 0.37761844493689467,\n",
       " 0.550760650956205,\n",
       " 0.7510497007667059,\n",
       " 0.4676555259434939,\n",
       " 0.8676248788822957,\n",
       " 0.43712001689046454,\n",
       " 0.8253628115642766,\n",
       " 0.4809472353965031,\n",
       " 0.806007700332146,\n",
       " 0.43882719744344734,\n",
       " 0.7081973259640855,\n",
       " 0.5008376560771547,\n",
       " 0.5200917130938079,\n",
       " 0.8121237455702093,\n",
       " 0.39383781245635385,\n",
       " 0.555204268297264,\n",
       " 0.5344529708162913,\n",
       " 0.5473409474707983,\n",
       " 0.8098099408736948,\n",
       " 0.8185899229506605,\n",
       " 0.2841551507559821,\n",
       " 0.6523598249087361,\n",
       " 0.8315657170958248,\n",
       " 0.4370069596387084,\n",
       " 0.440804373556569,\n",
       " 0.46475325978891513,\n",
       " 0.7956327112282173,\n",
       " 0.8063356959832548,\n",
       " 0.7802137600082244,\n",
       " 0.7900087183259127,\n",
       " 0.6427764665476083,\n",
       " 0.658121736186541,\n",
       " 0.6257664975487561,\n",
       " 0.38532326292261465,\n",
       " 0.8026197077208224,\n",
       " 0.7820967096889566,\n",
       " 0.469294334507276,\n",
       " 0.5215421911876261,\n",
       " 0.46891461170432625,\n",
       " 0.5037976740150129,\n",
       " 0.5465204812629736,\n",
       " 0.5722049648310474,\n",
       " 0.6013866203369587,\n",
       " 0.7214049747855384,\n",
       " 0.8930380189378527,\n",
       " 0.7783124057913197,\n",
       " 0.43680548395419627,\n",
       " 0.6913534504721107,\n",
       " 0.8342075925746936,\n",
       " 0.8287312091830905,\n",
       " 0.8360036538143356,\n",
       " 0.4006829341051149,\n",
       " 0.8405290536717926,\n",
       " 0.721904145530341,\n",
       " 0.8000116287372325,\n",
       " 0.8450749548962742,\n",
       " 0.4486286488812191,\n",
       " 0.6697631270318329,\n",
       " 0.41617378984171116,\n",
       " 0.44122157285327535,\n",
       " 0.7637049200179794,\n",
       " 0.3108519767340653,\n",
       " 0.49042287113650795,\n",
       " 0.3600460822114887,\n",
       " 0.36108685187066236,\n",
       " 0.8530306560225077,\n",
       " 0.8186783967187742,\n",
       " 0.45399527143003826,\n",
       " 0.34171989281923787,\n",
       " 0.8118878031042716,\n",
       " 0.5994878492192562,\n",
       " 0.38545767701760003,\n",
       " 0.7794848308580953,\n",
       " 0.7893099497887499,\n",
       " 0.5609773954433717,\n",
       " 0.3969129606766094,\n",
       " 0.5518678811770128,\n",
       " 0.5931012942630848,\n",
       " 0.4541118393837469,\n",
       " 0.476396350002408,\n",
       " 0.3093327616762175,\n",
       " 0.6141777749003088,\n",
       " 0.7681954365880616,\n",
       " 0.4049582550169839,\n",
       " 0.4806216230392293,\n",
       " 0.4951840265760198,\n",
       " 0.6762906389687029,\n",
       " 0.53869846315141,\n",
       " 0.5197515714639522,\n",
       " 0.8075442533475842,\n",
       " 0.6049190094602802,\n",
       " 0.5924317835841781,\n",
       " 0.7717718342434314,\n",
       " 0.7905096276038874,\n",
       " 0.5965107197925452,\n",
       " 0.7624075445789572,\n",
       " 0.4299497068181869,\n",
       " 0.6793599946782372,\n",
       " 0.4871147135615008,\n",
       " 0.5311898212481164,\n",
       " 0.3394780935678021,\n",
       " 0.8351728212583303,\n",
       " 0.4985296915636783,\n",
       " 0.8045952739251068,\n",
       " 0.8020937451214458,\n",
       " 0.8686002047332093,\n",
       " 0.8719554670578048,\n",
       " 0.6079979924365275,\n",
       " 0.6486253456408595,\n",
       " 0.6760045602058624,\n",
       " 0.6675826478953067,\n",
       " 0.6063932508535048,\n",
       " 0.3837433992193545,\n",
       " 0.6662940836295441,\n",
       " 0.37976841402254446,\n",
       " 0.6703214049854297,\n",
       " 0.42232589848412383,\n",
       " 0.5475658781169801,\n",
       " 0.40024902557982167,\n",
       " 0.5550171126732375,\n",
       " 0.5299971244255675,\n",
       " 0.8511557856444683,\n",
       " 0.3268691001861216,\n",
       " 0.8080481045210258,\n",
       " 0.46240518875296366,\n",
       " 0.3780868028903043,\n",
       " 0.34753304317132394,\n",
       " 0.6052843559388653,\n",
       " 0.8518661613547949,\n",
       " 0.8033106926661435,\n",
       " 0.3706223283132535,\n",
       " 0.46198332562033406,\n",
       " 0.410573156653526,\n",
       " 0.47507219338781503,\n",
       " 0.4569068746620756,\n",
       " 0.4731674992958297,\n",
       " 0.7875562313945939,\n",
       " 0.8178231534128219,\n",
       " 0.49415660368279213,\n",
       " 0.40939623103301004,\n",
       " 0.8047825198618881,\n",
       " 0.8147485028231879,\n",
       " 0.6476337241436927,\n",
       " 0.43458881047886955,\n",
       " 0.5075353979494963,\n",
       " 0.5953644910665601,\n",
       " 0.6489247511279694,\n",
       " 0.7993557441256689,\n",
       " 0.48013459993760155,\n",
       " 0.6671923780160569,\n",
       " 0.37460585910932065,\n",
       " 0.526625205234625,\n",
       " 0.45457313655474085,\n",
       " 0.8018575191929938,\n",
       " 0.4447601685623369,\n",
       " 0.5258511312386139,\n",
       " 0.3623407777345824,\n",
       " 0.8182279536105356,\n",
       " 0.6227587301132664,\n",
       " 0.8310673905569171,\n",
       " 0.45156724001919757,\n",
       " 0.4176744733601052,\n",
       " 0.740984135901227,\n",
       " 0.8357449867898792,\n",
       " 0.7949635483849248,\n",
       " 0.47452649021993776,\n",
       " 0.7979584203900137,\n",
       " 0.46697173866950703,\n",
       " 0.7455687108706477,\n",
       " 0.6087341864065334,\n",
       " 0.43115033226711963,\n",
       " 0.42861160130763654,\n",
       " 0.539260139466597,\n",
       " 0.4614306374182997,\n",
       " 0.3051599267680837,\n",
       " 0.46855457898376696,\n",
       " 0.6791513223822309,\n",
       " 0.44489211937312034,\n",
       " 0.45507214524353035,\n",
       " 0.3996960743576653,\n",
       " 0.5383304534335542,\n",
       " 0.7859281849119552,\n",
       " 0.3890682709547798,\n",
       " 0.414259145823486,\n",
       " 0.6216822527365202,\n",
       " 0.6099763574582435,\n",
       " 0.40846400624233925,\n",
       " 0.4297111863415316,\n",
       " 0.5194441931566187,\n",
       " 0.4645976198666435,\n",
       " 0.6044878835284739,\n",
       " 0.6327505132181183,\n",
       " 0.8113563026978674,\n",
       " 0.2885230246421324,\n",
       " 0.4240496443275962,\n",
       " 0.6124478604947817,\n",
       " 0.6927470379507494,\n",
       " 0.44251685031460763,\n",
       " 0.6492273576860539,\n",
       " 0.6372020728079153,\n",
       " 0.37706661321336865,\n",
       " 0.8071928301590797,\n",
       " 0.5699830260677388,\n",
       " 0.5551964868913626,\n",
       " 0.5950746175077295,\n",
       " 0.7866623800140811,\n",
       " 0.5007865056442848,\n",
       " 0.611390685667897,\n",
       " 0.6050403997673983,\n",
       " 0.22591613991241744,\n",
       " 0.3547286361722687,\n",
       " 0.8412703251663509,\n",
       " 0.5443028152174479,\n",
       " 0.5783611672655378,\n",
       " 0.5967712384793702,\n",
       " 0.8003896012384405,\n",
       " 0.6366771549473865,\n",
       " 0.3733442059617187,\n",
       " 0.8170766925397787,\n",
       " 0.6489764639412974,\n",
       " 0.7591729361303393,\n",
       " 0.49152281788365015,\n",
       " 0.5795056082733456,\n",
       " ...]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
