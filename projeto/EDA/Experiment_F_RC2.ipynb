{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment F\n",
    "\n",
    "2) Implementar um framework de busca de hiperparâmetros.\n",
    "\n",
    "2.1) Parâmetros específicos para cada método de processamento do ds.\n",
    "\n",
    "2.2) N top colunas (`top_cols`) do dataset.\n",
    "\n",
    "2.3) Parâmetro $L$ (`recomender(...,L,...)`).\n",
    "\n",
    "5) Implementar como entrada uma empresa nova, conter mapeamento de valores.\n",
    "\n",
    "Talvez criar o notebook da Second_View, com:\n",
    "1) Verificar `sklearn.inspection.permutation_importance`.\n",
    "\n",
    "- Author: Israel Oliveira [\\[e-mail\\]](mailto:'Israel%20Oliveira%20'<prof.israel@gmail.com>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NewType, List\n",
    "from loguru import logger\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.decomposition import FactorAnalysis, FastICA, PCA, IncrementalPCA, NMF, TruncatedSVD\n",
    "from collections import defaultdict, Counter\n",
    "import functools\n",
    "import operator\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas 1.0.5\n",
      "numpy  1.19.0\n",
      "2020-07-16 \n",
      "\n",
      "CPython 3.7.7\n",
      "IPython 7.16.1\n",
      "\n",
      "compiler   : GCC 8.3.0\n",
      "system     : Linux\n",
      "release    : 5.4.0-7634-generic\n",
      "machine    : x86_64\n",
      "processor  : \n",
      "CPU cores  : 8\n",
      "interpreter: 64bit\n",
      "Git hash   : 30ba73593d8a9aa656214e7030f2e3f09c6a4806\n",
      "Git repo   : https://github.com/ysraell/aceleradev_private.git\n",
      "Git branch : master\n"
     ]
    }
   ],
   "source": [
    "# Run this cell before close.\n",
    "%watermark -d --iversion -b -r -g -m -v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-16 22:29:38.238 | INFO     | __main__:<module>:1 - Carregando dataset...\n",
      "2020-07-16 22:29:46.895 | INFO     | __main__:<module>:6 - ...pronto!\n",
      "2020-07-16 22:29:46.895 | INFO     | __main__:<module>:8 - Carregando dataset de validação...\n",
      "2020-07-16 22:29:47.001 | INFO     | __main__:<module>:18 - ...pronto!\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Carregando dataset...\")\n",
    "\n",
    "path_data = '../data/'\n",
    "df_marked = pd.read_csv(path_data+'estaticos_market.csv')\n",
    "df_marked = df_marked.drop(columns=['Unnamed: 0'])\n",
    "logger.info(\"...pronto!\")\n",
    "\n",
    "logger.info(\"Carregando dataset de validação...\")\n",
    "\n",
    "df_ep_list = [pd.read_csv(path_data+'estaticos_portfolio{}.csv'.format(i+1)) for i in range(3)]\n",
    "tmp = []\n",
    "for i in range(3):\n",
    "    df_ep_list[i]['P'] = i+1 \n",
    "    tmp.append(df_ep_list[i][['id','P']])\n",
    "df_ep = pd.concat(tmp)\n",
    "del df_ep_list\n",
    "del tmp\n",
    "logger.info(\"...pronto!\")\n",
    "\n",
    "# Para desenvolvimento do framework:\n",
    "#df_marked = df_marked.merge(df_ep, on='id').drop(columns=['P'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-16 22:29:47.119 | INFO     | __main__:feat_proc:5 - Processando as features...\n",
      "2020-07-16 22:30:23.275 | INFO     | __main__:feat_proc:41 - ...pronto!\n"
     ]
    }
   ],
   "source": [
    "def flat(a):\n",
    "    return functools.reduce(operator.iconcat, a, []) \n",
    "\n",
    "def feat_proc(dataset = df_marked, col_target = 'id', feat_cols = df_marked.columns[2:], N_topcols = -1):\n",
    "    logger.info(\"Processando as features...\")\n",
    "    missing_count = {}\n",
    "    remove_cols = []\n",
    "    for col in feat_cols:\n",
    "        try:\n",
    "            missing_count[col] = sum(dataset[col].isna()) / dataset[col].nunique()\n",
    "            dataset[col] = dataset[col].fillna(0)*1\n",
    "        except ZeroDivisionError:\n",
    "            remove_cols.append(col)\n",
    "\n",
    "    feat_cols = [col for col in feat_cols if col not in remove_cols]\n",
    "\n",
    "    def normalize(x):\n",
    "        return (x-np.min(x))/(np.max(x) - np.min(x)) if (np.max(x) - np.min(x)) > 0 else (x-np.min(x))\n",
    "\n",
    "    for col in feat_cols:\n",
    "        try:\n",
    "            dataset[col] = normalize(dataset[col].tolist())\n",
    "        except:\n",
    "            maping = {val:i+1 for i,val in enumerate(dataset[col].unique())}\n",
    "            dataset[col] = dataset[col].apply(lambda x: maping[x])\n",
    "            dataset[col] = normalize(dataset[col].tolist())\n",
    "\n",
    "    remove_cols = []\n",
    "    for col in feat_cols:\n",
    "        if df_marked[col].nunique() == 1:\n",
    "            remove_cols.append(col)\n",
    "    feat_cols = [col for col in feat_cols if col not in remove_cols]\n",
    "    N_topcols = N_topcols if (N_topcols > 0) and (N_topcols <= len(feat_cols)) else -1\n",
    "    feat_cols_vals = [(col,val) for col,val in list(missing_count.items()) if col in feat_cols]\n",
    "    if N_topcols == -1:\n",
    "        top_cols = feat_cols\n",
    "    else:\n",
    "        top_cols = [col for col,_ in sorted(feat_cols_vals, key=lambda x: x[1])[:N_topcols]]\n",
    "    \n",
    "    missing_count = {key:val for key,val in missing_count.items() if col in feat_cols }\n",
    "    logger.info(\"...pronto!\")\n",
    "    return dataset[[col_target]+top_cols], missing_count\n",
    "\n",
    "\n",
    "\n",
    "def Manhattan(X,vec):\n",
    "    return abs(X - vec).sum(-1)\n",
    "\n",
    "def Camberra(X,vec):\n",
    "    return abs((X - vec)/(X + vec)).sum(-1)\n",
    "\n",
    "def BrayCurtis(X,vec):\n",
    "    return abs((X - vec)).sum(-1) / abs((X - vec)).sum(-1).sum(-1)\n",
    "\n",
    "def np_cossine(X,vec):\n",
    "    return np.array([sum(X[i]*vec) / sum(X[i]**2)*sum(vec**2) for i in range(X.shape[0])])\n",
    "\n",
    "def npj_cossine(X,vec):\n",
    "    return npj.array([sum(X[i]*vec) / sum(X[i]**2)*sum(vec**2) for i in range(X.shape[0])])\n",
    "\n",
    "def scy_cossine(X,vec):\n",
    "    return np.array([cosine(X[i],vec) for i in range(X.shape[0])])\n",
    "\n",
    "#Manhattanj = jit(Manhattan)\n",
    "#Camberraj = jit(Camberra)\n",
    "#BrayCurtisj = jit(BrayCurtis)\n",
    "#np_cossinej = jit(npj_cossine)\n",
    "\n",
    "dist_func = [Manhattan, Camberra, BrayCurtis, np_cossine, scy_cossine]\n",
    "#tmp = [Manhattanj, Camberraj, BrayCurtisj, np_cossinej]\n",
    "\n",
    "#for dist in tmp:\n",
    "#    dist.__name__ += 'j'\n",
    "#\n",
    "#dist_func = dist_func+tmp\n",
    "#del tmp\n",
    "\n",
    "def Nothing(arg):\n",
    "    return arg\n",
    "\n",
    "def npSVD(M):\n",
    "    u, _, _ = np.linalg.svd(M, full_matrices=False)\n",
    "    return u\n",
    "\n",
    "#def npSVDj(M):\n",
    "#    u, _, _ = npj.linalg.svd(M, full_matrices=False)\n",
    "#    return u\n",
    "\n",
    "# Mais rápido!\n",
    "#_npSVDj = jit(npSVDj)\n",
    "\n",
    "\n",
    "def _PCA(M,n_components=None):\n",
    "    out = PCA(n_components=n_components)\n",
    "    return out.fit_transform(M)\n",
    "\n",
    "def _FastICA(M,n_components=None):\n",
    "    out = FastICA(n_components=n_components)\n",
    "    return out.fit_transform(M)\n",
    "\n",
    "def _FactorAnalysis(M,n_components=None):\n",
    "    out = FactorAnalysis(n_components=n_components)\n",
    "    return out.fit_transform(M)\n",
    "\n",
    "def _IncrementalPCA(M,n_components=None):\n",
    "    out = IncrementalPCA(n_components=n_components)\n",
    "    return out.fit_transform(M)\n",
    "\n",
    "def _TruncatedSVD(M,n_components=None):\n",
    "    out = TruncatedSVD(n_components=n_components)\n",
    "    return out.fit_transform(M)\n",
    "\n",
    "def _NMF(M,n_components=None):\n",
    "    out = NMF(n_components=n_components)\n",
    "    return out.fit_transform(M)\n",
    "\n",
    "redux_func = [Nothing, npSVD, _NMF, _TruncatedSVD, _IncrementalPCA, _FactorAnalysis, _FastICA, _PCA]\n",
    "\n",
    "data, missing_count = feat_proc()\n",
    "\n",
    "def escalaropt_missings(df: pd.DataFrame, score: dict):\n",
    "    df_score = pd.DataFrame(score.items(), columns=['col','score'])\n",
    "    df_score = pd.DataFrame(missing_count.items(), columns=['col','score'])\n",
    "    df_score['escala_opt'] = 1-normalize((np.sqrt(df_score.score)))\n",
    "    #df_score['escala_opt'].sort_values().reset_index(drop=True).plot()\n",
    "    #df_score['escala_opt'].apply(lambda x: max(x,0.1)).sort_values().reset_index(drop=True).plot()\n",
    "    df_score['escala_opt'] = df_score['escala_opt'].apply(lambda x: max(x,0.1))\n",
    "    for row in df_score.iterrows():\n",
    "        df[row.col] = row.escala_opt*df[row.col]\n",
    "    return df\n",
    "\n",
    "def escalaropt_std(df: pd.DataFrame, score: dict):\n",
    "    df_score = pd.DataFrame(score.items(), columns=['col','score'])\n",
    "    df_score = pd.DataFrame(missing_count.items(), columns=['col','score'])\n",
    "    df_score['escala_opt'] = normalize([np.sqrt(np.sqrt(np.sqrt(df[col].std()))) for col in df_score['col']])\n",
    "    #df_score['escala_opt'].sort_values().reset_index(drop=True).plot()\n",
    "    #df_score['escala_opt'].apply(lambda x: max(x,0.1)).sort_values().reset_index(drop=True).plot()\n",
    "    df_score['escala_opt'] = df_score['escala_opt'].apply(lambda x: max(x,0.1))\n",
    "    for row in df_score.iterrows():\n",
    "        df[row.col] = row.escala_opt*df[row.col]\n",
    "    return df\n",
    "\n",
    "def escalaropt_entropy(df: pd.DataFrame, score: dict):\n",
    "    df_score = pd.DataFrame(score.items(), columns=['col','score'])\n",
    "    df_score = pd.DataFrame(missing_count.items(), columns=['col','score'])\n",
    "    df_score['escala_opt'] = normalize([(-sum((df[col]+1)*np.log(df[col]+1))) for col in df_score['col']])\n",
    "    #df_score['escala_opt'].sort_values().reset_index(drop=True).plot()\n",
    "    #df_score['escala_opt'].apply(lambda x: max(x,0.1)).sort_values().reset_index(drop=True).plot()\n",
    "    df_score['escala_opt'] = df_score['escala_opt'].apply(lambda x: max(x,0.1))\n",
    "    for row in df_score.iterrows():\n",
    "        df[row.col] = row.escala_opt*df[row.col]\n",
    "    return df\n",
    "\n",
    "procDS_func = [Nothing, escalaropt_missings, escalaropt_std, escalaropt_entropy]\n",
    "\n",
    "Uid = NewType('uid', int)\n",
    "Raw = NewType('raw', str)\n",
    "\n",
    "class ExMatrix():\n",
    "    \"\"\"\n",
    "        ************\n",
    "    \"\"\"\n",
    "    def __init__(self,process_values = Nothing, factorize = Nothing, vector_distance = Manhattan, stateless: bool = False):\n",
    "        self.matrix_dict = {}\n",
    "        self.stateless = stateless\n",
    "        self.M = None\n",
    "        self.pu = None\n",
    "        self.raw = None\n",
    "        self.uid = None\n",
    "        self.vector_distance = vector_distance\n",
    "        self.factorize = factorize\n",
    "        self.process_values = Nothing\n",
    "\n",
    "    def fit(self,dataset: pd.DataFrame):\n",
    "        \"\"\"\n",
    "            ...\n",
    "        \"\"\"\n",
    "        self.raw = dataset[dataset.columns[0]].to_dict()\n",
    "        self.uid = {raw:uid for uid,raw in self.raw.items()}\n",
    "        self.all_raw = dataset[dataset.columns[0]].tolist()\n",
    "        self.all_uid = dataset.index\n",
    "        dataset = self.process_values(dataset)\n",
    "        ds_size = len(dataset[dataset.columns[1:]].values)\n",
    "        self.M = self.factorize(dataset[dataset.columns[1:]].values)\n",
    "        if ds_size != self.M.shape[0]:\n",
    "            raise ValueError('A fatoração não está correta!')\n",
    "        del dataset\n",
    "        \n",
    "    def _get_neighbors(self,uid: Uid, k: int = 1, black_list: List[Uid] = []) -> List[Uid]:\n",
    "        \"\"\"\n",
    "            Calcula todas as distâncias entre 'uid' de entrada e todos os outros 'uid'.\n",
    "            A distância calciulada é armazenda e não calculada novamente. \n",
    "        \"\"\"\n",
    "        k = k if k >= 0 else 0\n",
    "        #logger.info(\"Calculando todos os vizinhos...\")\n",
    "        #for uid2 in tqdm(self.trainset.all_users()):\n",
    "        if uid not in self.matrix_dict.keys():\n",
    "            self.matrix_dict[uid] = self.vector_distance(self.M,self.M[uid])\n",
    "        out = [x[0] for x in sorted(\n",
    "            [\n",
    "                (uid2, self.matrix_dict[uid][uid2])\n",
    "                for uid2 in self.all_uid\n",
    "                if (uid2 not in black_list)\n",
    "            ], key=lambda x: x[1])][:k]\n",
    "        if self.stateless:\n",
    "            del self.matrix_dict\n",
    "            self.matrix_dict = {}\n",
    "        return out\n",
    "    \n",
    "    def _uid2raw(self, uid: Uid)-> str:\n",
    "        '''\n",
    "            uid -> raw.\n",
    "            Valor interno para externo, o nome original do usuário.\n",
    "        '''\n",
    "        return self.raw[uid]\n",
    "    \n",
    "    def _raw2uid(self, raw: Raw)-> int:\n",
    "        '''\n",
    "            raw -> uid.\n",
    "            Valor externo para interno, o id interno do usuários..\n",
    "        '''\n",
    "        return self.uid[raw]\n",
    "    \n",
    "    def recomender(self, in_list: List[Raw], k: int = 1, L: int = 3, Fk: int = 1, limit: int = 10)-> List[Raw]:\n",
    "        '''\n",
    "            Faz as recomendacoes.\n",
    "            ##### Função incompleta #####\n",
    "        '''\n",
    "        # Pega quantas recomendações por usuário em `in_list`,\n",
    "        # mas sem deixar faltar\n",
    "        N_in = len(in_list)\n",
    "        k = k if k > 0 else 1\n",
    "        R_per_in = L*(k//N_in + min(k%N_in,1))\n",
    "\n",
    "        # Pega os `uid`\n",
    "        uid_in_list = [self._raw2uid(raw) for raw in in_list]\n",
    "\n",
    "        # Pega os vizinhos mais próximos de cada uid de entrada.\n",
    "        done = False\n",
    "        flag = True\n",
    "        Rounds = 0\n",
    "        while limit and (not done):\n",
    "            Rounds += 1\n",
    "            # Ele sempre pega todos novamente.\n",
    "            recomendations_list = [self._get_neighbors(uid,R_per_in,uid_in_list) for uid in uid_in_list]\n",
    "            # Quando limit = 0, encerra.\n",
    "            limit -= 1\n",
    "            # Quando tem gente o suficiente, encerra.\n",
    "            if len(set(flat(recomendations_list))) >= Fk*k:\n",
    "                done = True\n",
    "            # Depois do primeiro loop, pega um a mais.\n",
    "            R_per_in += 1\n",
    "\n",
    "        # Aqui gera um dicionário ordenando por votacao.\n",
    "        count_rec = Counter(flat(recomendations_list)) # A votação!!\n",
    "        count_rec = list(count_rec.items())\n",
    "        ct_pos = defaultdict(list)\n",
    "        #ct_pos_inv = defaultdict(list)\n",
    "        while count_rec:\n",
    "            tmp = count_rec.pop(0)\n",
    "            ct_pos[tmp[1]].append(tmp[0])\n",
    "            #ct_pos_inv[tmp[0]].append(tmp[1])\n",
    "\n",
    "        # Aqui considera a posiçao de vizinhos mais proximos.\n",
    "        #nn_pos = defaultdict(list)\n",
    "        nn_pos_inv = defaultdict(list)\n",
    "        tmp = deepcopy(recomendations_list)\n",
    "        while tmp:\n",
    "            tmp2 = tmp.pop(0)\n",
    "            n = 0\n",
    "            while tmp2:\n",
    "                n += 1\n",
    "                tmp3 = tmp2.pop(0)\n",
    "                #nn_pos[n].append(tmp3)\n",
    "                nn_pos_inv[tmp3].append(n)\n",
    "\n",
    "        # Vai separando por votação e ordem de proximidade como desempate.      \n",
    "        votos_list = list(ct_pos.keys())\n",
    "        out_uid = []\n",
    "        while votos_list and k:\n",
    "            votos = max(votos_list)\n",
    "            votos_list.remove(votos)\n",
    "            tmp = sorted([(tmp, min(nn_pos_inv[tmp])) for tmp in ct_pos[votos]], key=lambda x: x[1])\n",
    "            while tmp and k:\n",
    "                out_uid.append(tmp.pop(0)[0])\n",
    "                k -= 1\n",
    "\n",
    "        # converte para Raw e \"joga fora\".\n",
    "        return [self._uid2raw(uid) for uid in out_uid]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Search(N=1, process_values = Nothing, factorize = Nothing, vector_distance_list = [Manhattan]):\n",
    "    ex_algo = ExMatrix(process_values = process_values, factorize = factorize)\n",
    "    ex_algo.fit(data)\n",
    "\n",
    "    out = {}\n",
    "    for dist in vector_distance_list:\n",
    "        ex_algo.vector_distance = dist\n",
    "        print(dist.__name__)\n",
    "        tmp ={1: [0], 2: [0], 3: [0]}\n",
    "        t = time()\n",
    "        for row in tqdm(df_ep.iterrows()):\n",
    "            recs = ex_algo.recomender([row[1].id],k=N)\n",
    "            tmp[row[1].P].append(any([x in df_ep.loc[df_ep.P == row[1].P].id.to_list() for x in recs])*1)\n",
    "        t = time()-t\n",
    "        out[dist.__name__] = {i: (sum(val)/max(1,len(val)), sum(val), len(val)) for i,val in tmp.items()}\n",
    "        out[dist.__name__]['t'] = t\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_list = [BrayCurtis] #[Manhattan, scy_cossine, Camberra, BrayCurtis, np_cossine] #[Manhattan, Camberra, BrayCurtis] #dist_func\n",
    "proc_list = procDS_func #procDS_func #[Nothing] #procDS_func\n",
    "redux_list = [ _FastICA] #redux_func #[Nothing] #[Nothing, npSVD, _NMF, _PCA, _FactorAnalysis] #redux_func\n",
    "n_components_dict = {Nothing.__name__ : False,\n",
    "                  #_npSVDj.__name__: False,\n",
    "                  npSVD.__name__: False,\n",
    "                  _NMF.__name__ : True,\n",
    "                  _TruncatedSVD.__name__ : True,\n",
    "                  _IncrementalPCA.__name__ : True,\n",
    "                  _FactorAnalysis.__name__ : True,\n",
    "                  _FastICA.__name__ : True,\n",
    "                  _PCA.__name__ : True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_csv_name = 'Results_RC2.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_laoded = pd.read_csv(results_csv_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(results,df_e):\n",
    "    df = pd.DataFrame(results, columns=['pre_proc','redux_func','n_components','dist','t','P1pp','P1_True','P1_len','P2pp','P2_True','P2_len','P3pp','P3_True','P3_len'])\n",
    "    out = pd.concat([df_e, df])\n",
    "    #df_laoded.df_laodedto_csv('Results_redux_prepro.csv',index=False)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<function __main__.Nothing(arg)>,\n",
       " <function __main__.escalaropt_missings(df: pandas.core.frame.DataFrame, score: dict)>,\n",
       " <function __main__.escalaropt_std(df: pandas.core.frame.DataFrame, score: dict)>,\n",
       " <function __main__.escalaropt_entropy(df: pandas.core.frame.DataFrame, score: dict)>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Done?: ['Nothing', '_FastICA', 10].\n",
      "Done: ['Nothing', '_FastICA', 10].\n",
      "Done?: ['Nothing', '_FastICA', 20].\n",
      "Done: ['Nothing', '_FastICA', 20].\n",
      "Done?: ['Nothing', '_FastICA', 30].\n",
      "Done: ['Nothing', '_FastICA', 30].\n",
      "Done?: ['Nothing', '_FastICA', 35].\n",
      "Done: ['Nothing', '_FastICA', 35].\n",
      "Done?: ['Nothing', '_FastICA', 40].\n",
      "Done: ['Nothing', '_FastICA', 40].\n",
      "Done?: ['Nothing', '_FastICA', 45].\n",
      "Done: ['Nothing', '_FastICA', 45].\n",
      "Done?: ['Nothing', '_FastICA', 50].\n",
      "Done: ['Nothing', '_FastICA', 50].\n",
      "Done?: ['Nothing', '_FastICA', 55].\n",
      "Done: ['Nothing', '_FastICA', 55].\n",
      "Done?: ['Nothing', '_FastICA', 60].\n",
      "Done: ['Nothing', '_FastICA', 60].\n",
      "Done?: ['Nothing', '_FastICA', 65].\n",
      "Done: ['Nothing', '_FastICA', 65].\n",
      "Done?: ['Nothing', '_FastICA', 70].\n",
      "Done: ['Nothing', '_FastICA', 70].\n",
      "Done?: ['Nothing', '_FastICA', 80].\n",
      "Done: ['Nothing', '_FastICA', 80].\n",
      "Done?: ['Nothing', '_FastICA', 100].\n",
      "Done: ['Nothing', '_FastICA', 100].\n",
      "Done?: ['Nothing', '_FastICA', 120].\n",
      "Done: ['Nothing', '_FastICA', 120].\n",
      "Done?: ['Nothing', '_FastICA', 150].\n",
      "Done: ['Nothing', '_FastICA', 150].\n",
      "2\n",
      "Done?: ['escalaropt_missings', '_FastICA', 10].\n",
      "Done: ['escalaropt_missings', '_FastICA', 10].\n",
      "Done?: ['escalaropt_missings', '_FastICA', 20].\n",
      "Done: ['escalaropt_missings', '_FastICA', 20].\n",
      "Done?: ['escalaropt_missings', '_FastICA', 30].\n",
      "Done: ['escalaropt_missings', '_FastICA', 30].\n",
      "Done?: ['escalaropt_missings', '_FastICA', 35].\n",
      "Done: ['escalaropt_missings', '_FastICA', 35].\n",
      "Done?: ['escalaropt_missings', '_FastICA', 40].\n",
      "Done: ['escalaropt_missings', '_FastICA', 40].\n",
      "Done?: ['escalaropt_missings', '_FastICA', 45].\n",
      "Done: ['escalaropt_missings', '_FastICA', 45].\n",
      "Done?: ['escalaropt_missings', '_FastICA', 50].\n",
      "Done: ['escalaropt_missings', '_FastICA', 50].\n",
      "Done?: ['escalaropt_missings', '_FastICA', 55].\n",
      "Done: ['escalaropt_missings', '_FastICA', 55].\n",
      "Done?: ['escalaropt_missings', '_FastICA', 60].\n",
      "Done: ['escalaropt_missings', '_FastICA', 60].\n",
      "Done?: ['escalaropt_missings', '_FastICA', 65].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BrayCurtis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1386it [22:08,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done?: ['escalaropt_missings', '_FastICA', 70].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BrayCurtis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1386it [22:36,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done?: ['escalaropt_missings', '_FastICA', 80].\n",
      "Done: ['escalaropt_missings', '_FastICA', 80].\n",
      "Done?: ['escalaropt_missings', '_FastICA', 100].\n",
      "Done: ['escalaropt_missings', '_FastICA', 100].\n",
      "Done?: ['escalaropt_missings', '_FastICA', 120].\n",
      "Done: ['escalaropt_missings', '_FastICA', 120].\n",
      "Done?: ['escalaropt_missings', '_FastICA', 150].\n",
      "Done: ['escalaropt_missings', '_FastICA', 150].\n",
      "3\n",
      "Done?: ['escalaropt_std', '_FastICA', 10].\n",
      "Done: ['escalaropt_std', '_FastICA', 10].\n",
      "Done?: ['escalaropt_std', '_FastICA', 20].\n",
      "Done: ['escalaropt_std', '_FastICA', 20].\n",
      "Done?: ['escalaropt_std', '_FastICA', 30].\n",
      "Done: ['escalaropt_std', '_FastICA', 30].\n",
      "Done?: ['escalaropt_std', '_FastICA', 35].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BrayCurtis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1386it [19:37,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done?: ['escalaropt_std', '_FastICA', 40].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BrayCurtis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1386it [20:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done?: ['escalaropt_std', '_FastICA', 45].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BrayCurtis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1386it [20:23,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done?: ['escalaropt_std', '_FastICA', 50].\n",
      "Done: ['escalaropt_std', '_FastICA', 50].\n",
      "Done?: ['escalaropt_std', '_FastICA', 55].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BrayCurtis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1386it [21:17,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done?: ['escalaropt_std', '_FastICA', 60].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BrayCurtis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1386it [21:42,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done?: ['escalaropt_std', '_FastICA', 65].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BrayCurtis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1386it [22:08,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done?: ['escalaropt_std', '_FastICA', 70].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BrayCurtis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1386it [22:39,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done?: ['escalaropt_std', '_FastICA', 80].\n",
      "Done: ['escalaropt_std', '_FastICA', 80].\n",
      "Done?: ['escalaropt_std', '_FastICA', 100].\n",
      "Done: ['escalaropt_std', '_FastICA', 100].\n",
      "Done?: ['escalaropt_std', '_FastICA', 120].\n",
      "Done: ['escalaropt_std', '_FastICA', 120].\n",
      "Done?: ['escalaropt_std', '_FastICA', 150].\n",
      "Done: ['escalaropt_std', '_FastICA', 150].\n",
      "4\n",
      "Done?: ['escalaropt_entropy', '_FastICA', 10].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BrayCurtis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1386it [17:07,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done?: ['escalaropt_entropy', '_FastICA', 20].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BrayCurtis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1386it [18:09,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done?: ['escalaropt_entropy', '_FastICA', 30].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BrayCurtis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1386it [19:05,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done?: ['escalaropt_entropy', '_FastICA', 35].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BrayCurtis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1386it [19:37,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done?: ['escalaropt_entropy', '_FastICA', 40].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BrayCurtis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1386it [19:57,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done?: ['escalaropt_entropy', '_FastICA', 45].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BrayCurtis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1386it [20:24,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done?: ['escalaropt_entropy', '_FastICA', 50].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BrayCurtis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1386it [20:49,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done?: ['escalaropt_entropy', '_FastICA', 55].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BrayCurtis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1386it [21:17,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done?: ['escalaropt_entropy', '_FastICA', 60].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BrayCurtis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1386it [21:42,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done?: ['escalaropt_entropy', '_FastICA', 65].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BrayCurtis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1386it [22:07,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done?: ['escalaropt_entropy', '_FastICA', 70].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BrayCurtis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1386it [22:34,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done?: ['escalaropt_entropy', '_FastICA', 80].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BrayCurtis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1386it [23:37,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done?: ['escalaropt_entropy', '_FastICA', 100].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/decomposition/_fastica.py:120: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  ConvergenceWarning)\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BrayCurtis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1386it [25:09,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done?: ['escalaropt_entropy', '_FastICA', 120].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BrayCurtis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1386it [26:45,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done?: ['escalaropt_entropy', '_FastICA', 150].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BrayCurtis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1386it [29:14,  1.27s/it]\n"
     ]
    }
   ],
   "source": [
    "n_components_list = [n in range(30,80,2)]+[n in range(80,150,5)]\n",
    "n=0\n",
    "for proc in proc_list:\n",
    "    n += 1\n",
    "    print(n)\n",
    "    for redux in redux_list:\n",
    "        if n_components_dict[redux.__name__]:\n",
    "            for n_components in n_components_list:\n",
    "                cond = (df_laoded['pre_proc'] == proc.__name__) & (df_laoded['redux_func'] == redux.__name__) & (df_laoded['n_components'] == n_components)\n",
    "                print(\"Done?: {}.\".format([proc.__name__, redux.__name__, n_components]))\n",
    "                if sum(cond) == 0:\n",
    "                    def redux_tmp(M):\n",
    "                        return redux(M,n_components=n_components)\n",
    "                    tmp = Search(process_values = proc, factorize = redux_tmp, vector_distance_list = dist_list)\n",
    "                    results = [[proc.__name__, redux.__name__, n_components] + r for r in [[key]+[tmp[key]['t']]+flat([list(tmp[key][i+1]) for i in range(3)]) for key in tmp.keys()]]\n",
    "                    df_laoded = save_results(results,df_laoded)\n",
    "                    df_laoded.to_csv(results_csv_name,index=False)\n",
    "                else:\n",
    "                    print(\"Done: {}.\".format([proc.__name__, redux.__name__, n_components]))\n",
    "        else:\n",
    "            n_components = data.shape[1]\n",
    "            cond = (df_laoded['pre_proc'] == proc.__name__) & (df_laoded['redux_func'] == redux.__name__) & (df_laoded['n_components'] == n_components)\n",
    "            print(\"Done?: {}.\".format([proc.__name__, redux.__name__, n_components]))\n",
    "            if sum(cond) == 0:\n",
    "                tmp = Search(process_values = proc, factorize = redux, vector_distance_list= dist_list)\n",
    "                results = [[proc.__name__, redux.__name__, n_components] + r for r in [[key]+[tmp[key]['t']]+flat([list(tmp[key][i+1]) for i in range(3)]) for key in tmp.keys()]]\n",
    "                df_laoded = save_results(results,df_laoded)\n",
    "                df_laoded.to_csv(results_csv_name,index=False)\n",
    "            else:\n",
    "                print(\"Done: {}.\".format([proc.__name__, redux.__name__, n_components]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['pre_proc','redux_func','n_components','dist','t','P1pp','P1_True','P1_len','P2pp','P2_True','P2_len','P3pp','P3_True','P3_len'])\n",
    "df.to_csv(results_csv_name,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pre_proc</th>\n",
       "      <th>redux_func</th>\n",
       "      <th>n_components</th>\n",
       "      <th>dist</th>\n",
       "      <th>t</th>\n",
       "      <th>P1pp</th>\n",
       "      <th>P1_True</th>\n",
       "      <th>P1_len</th>\n",
       "      <th>P2pp</th>\n",
       "      <th>P2_True</th>\n",
       "      <th>P2_len</th>\n",
       "      <th>P3pp</th>\n",
       "      <th>P3_True</th>\n",
       "      <th>P3_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>escalaropt_missings</td>\n",
       "      <td>Nothing</td>\n",
       "      <td>168</td>\n",
       "      <td>np_cossine</td>\n",
       "      <td>1005.226768</td>\n",
       "      <td>0.005396</td>\n",
       "      <td>3</td>\n",
       "      <td>556</td>\n",
       "      <td>0.253968</td>\n",
       "      <td>144</td>\n",
       "      <td>567</td>\n",
       "      <td>0.251880</td>\n",
       "      <td>67</td>\n",
       "      <td>266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>escalaropt_missings</td>\n",
       "      <td>Nothing</td>\n",
       "      <td>168</td>\n",
       "      <td>Camberra</td>\n",
       "      <td>1006.786389</td>\n",
       "      <td>0.005396</td>\n",
       "      <td>3</td>\n",
       "      <td>556</td>\n",
       "      <td>0.253968</td>\n",
       "      <td>144</td>\n",
       "      <td>567</td>\n",
       "      <td>0.251880</td>\n",
       "      <td>67</td>\n",
       "      <td>266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>escalaropt_missings</td>\n",
       "      <td>npSVD</td>\n",
       "      <td>168</td>\n",
       "      <td>Camberra</td>\n",
       "      <td>1008.371484</td>\n",
       "      <td>0.001799</td>\n",
       "      <td>1</td>\n",
       "      <td>556</td>\n",
       "      <td>0.204586</td>\n",
       "      <td>116</td>\n",
       "      <td>567</td>\n",
       "      <td>0.274436</td>\n",
       "      <td>73</td>\n",
       "      <td>266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>escalaropt_missings</td>\n",
       "      <td>npSVD</td>\n",
       "      <td>168</td>\n",
       "      <td>BrayCurtis</td>\n",
       "      <td>1017.122522</td>\n",
       "      <td>0.001799</td>\n",
       "      <td>1</td>\n",
       "      <td>556</td>\n",
       "      <td>0.204586</td>\n",
       "      <td>116</td>\n",
       "      <td>567</td>\n",
       "      <td>0.274436</td>\n",
       "      <td>73</td>\n",
       "      <td>266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>escalaropt_missings</td>\n",
       "      <td>Nothing</td>\n",
       "      <td>168</td>\n",
       "      <td>BrayCurtis</td>\n",
       "      <td>1023.607082</td>\n",
       "      <td>0.005396</td>\n",
       "      <td>3</td>\n",
       "      <td>556</td>\n",
       "      <td>0.253968</td>\n",
       "      <td>144</td>\n",
       "      <td>567</td>\n",
       "      <td>0.251880</td>\n",
       "      <td>67</td>\n",
       "      <td>266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>escalaropt_missings</td>\n",
       "      <td>npSVD</td>\n",
       "      <td>168</td>\n",
       "      <td>np_cossine</td>\n",
       "      <td>1029.013836</td>\n",
       "      <td>0.001799</td>\n",
       "      <td>1</td>\n",
       "      <td>556</td>\n",
       "      <td>0.204586</td>\n",
       "      <td>116</td>\n",
       "      <td>567</td>\n",
       "      <td>0.274436</td>\n",
       "      <td>73</td>\n",
       "      <td>266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>escalaropt_missings</td>\n",
       "      <td>Nothing</td>\n",
       "      <td>168</td>\n",
       "      <td>scy_cossine</td>\n",
       "      <td>1037.101138</td>\n",
       "      <td>0.005396</td>\n",
       "      <td>3</td>\n",
       "      <td>556</td>\n",
       "      <td>0.253968</td>\n",
       "      <td>144</td>\n",
       "      <td>567</td>\n",
       "      <td>0.251880</td>\n",
       "      <td>67</td>\n",
       "      <td>266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>escalaropt_missings</td>\n",
       "      <td>npSVD</td>\n",
       "      <td>168</td>\n",
       "      <td>scy_cossine</td>\n",
       "      <td>1062.770353</td>\n",
       "      <td>0.001799</td>\n",
       "      <td>1</td>\n",
       "      <td>556</td>\n",
       "      <td>0.204586</td>\n",
       "      <td>116</td>\n",
       "      <td>567</td>\n",
       "      <td>0.274436</td>\n",
       "      <td>73</td>\n",
       "      <td>266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>escalaropt_missings</td>\n",
       "      <td>Nothing</td>\n",
       "      <td>168</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>1431.732030</td>\n",
       "      <td>0.005396</td>\n",
       "      <td>3</td>\n",
       "      <td>556</td>\n",
       "      <td>0.253968</td>\n",
       "      <td>144</td>\n",
       "      <td>567</td>\n",
       "      <td>0.251880</td>\n",
       "      <td>67</td>\n",
       "      <td>266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>escalaropt_missings</td>\n",
       "      <td>npSVD</td>\n",
       "      <td>168</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>1772.359701</td>\n",
       "      <td>0.001799</td>\n",
       "      <td>1</td>\n",
       "      <td>556</td>\n",
       "      <td>0.204586</td>\n",
       "      <td>116</td>\n",
       "      <td>567</td>\n",
       "      <td>0.274436</td>\n",
       "      <td>73</td>\n",
       "      <td>266</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              pre_proc redux_func n_components         dist            t  \\\n",
       "4  escalaropt_missings    Nothing          168   np_cossine  1005.226768   \n",
       "2  escalaropt_missings    Nothing          168     Camberra  1006.786389   \n",
       "2  escalaropt_missings      npSVD          168     Camberra  1008.371484   \n",
       "3  escalaropt_missings      npSVD          168   BrayCurtis  1017.122522   \n",
       "3  escalaropt_missings    Nothing          168   BrayCurtis  1023.607082   \n",
       "4  escalaropt_missings      npSVD          168   np_cossine  1029.013836   \n",
       "1  escalaropt_missings    Nothing          168  scy_cossine  1037.101138   \n",
       "1  escalaropt_missings      npSVD          168  scy_cossine  1062.770353   \n",
       "0  escalaropt_missings    Nothing          168    Manhattan  1431.732030   \n",
       "0  escalaropt_missings      npSVD          168    Manhattan  1772.359701   \n",
       "\n",
       "       P1pp P1_True P1_len      P2pp P2_True P2_len      P3pp P3_True P3_len  \n",
       "4  0.005396       3    556  0.253968     144    567  0.251880      67    266  \n",
       "2  0.005396       3    556  0.253968     144    567  0.251880      67    266  \n",
       "2  0.001799       1    556  0.204586     116    567  0.274436      73    266  \n",
       "3  0.001799       1    556  0.204586     116    567  0.274436      73    266  \n",
       "3  0.005396       3    556  0.253968     144    567  0.251880      67    266  \n",
       "4  0.001799       1    556  0.204586     116    567  0.274436      73    266  \n",
       "1  0.005396       3    556  0.253968     144    567  0.251880      67    266  \n",
       "1  0.001799       1    556  0.204586     116    567  0.274436      73    266  \n",
       "0  0.005396       3    556  0.253968     144    567  0.251880      67    266  \n",
       "0  0.001799       1    556  0.204586     116    567  0.274436      73    266  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_laoded.sort_values(by='t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_laoded.to_csv(results_csv_name,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1000*44.334993/17.946285"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
