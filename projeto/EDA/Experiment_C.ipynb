{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment C \n",
    "\n",
    "Ideas:\n",
    "\n",
    "~~1) Estudar as diferentes distâncias para vetores 1-D em `scipy.spatial.distance`.~~\n",
    "\n",
    "~~2) De 1, implementar e otimizar os que melhor performam comos portfólios.~~\n",
    "\n",
    "3) Ver a possibilidade de recomendar com base em 2 ou mais empresas (`users`)\n",
    "\n",
    "Eu não queria usar valores médios entre os vetores ou representantes de clusters,\n",
    "pois creio que a distorção espacial ao calcular um vetor médio pode ser maior quanto\n",
    "mais empresas eu tiver de entrada. Penso que uma abordagem interessante é que garante\n",
    "pelo menos uma proximidade maior entre as empresas é por votação. Via clsuter e vetor\n",
    "médio eu faço recomendações que não são as mais próximas de todos, mas nem tão distantes,\n",
    "mas isso pode excluir os potenciais melhores recomendações. Com votação, eu aumento as \n",
    "chances de recomendar empresas que são muito próximas para umas, ainda que seja distante\n",
    "de outras. \n",
    "\n",
    "4) Validar as recomendações com os portfólios.\n",
    "\n",
    "Como utilizar os portfólios dados para validar o modelo? Não sabe-se a ordem em que os\n",
    "portfólios foram crescendo. Então fazer um-a-um e N-a-N. Recomendações 1-para-1, 1-para-N, N-para-1 e N-para-N.\n",
    "\n",
    "Estou pensando em uma métrica que trate de forma acumulada, p.e.:\n",
    "\n",
    "- **A) 1-para-1:** Para cada uma empresa dentro de cada portfólio, eu vejo se a recomendação já está dentro do portfólio.\n",
    "\n",
    "- **B) 1-para-N:** Para cada uma empresa dentro de cada portfólio, requisito N recomendações. Verifico quantas das N recomendações está no portfólio.\n",
    "\n",
    "- **C) N-para-1:** Amostro N empresas e requisito 1 recomendação e confiro se está dentro do portfólio. Repito isso uma M vezes.\n",
    "\n",
    "- **D) N-para-N:** Amostro N empresas e requisito N recomendação e confiro se está dentro do portfólio. Repito isso uma M vezes. \n",
    "\n",
    "\n",
    "*Author: Israel Oliveira [\\[e-mail\\]](mailto:'Israel%20Oliveira%20'<prof.israel@gmail.com>)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from surprise import SVD, accuracy, Dataset, Reader\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "from loguru import logger\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-21T17:35:21+00:00\n",
      "\n",
      "CPython 3.7.7\n",
      "IPython 7.15.0\n",
      "\n",
      "compiler   : GCC 8.3.0\n",
      "system     : Linux\n",
      "release    : 4.19.76-linuxkit\n",
      "machine    : x86_64\n",
      "processor  : \n",
      "CPU cores  : 16\n",
      "interpreter: 64bit\n",
      "loguru 0.5.1\n",
      "scipy 1.4.1\n",
      "surprise 0.1\n",
      "numpy  1.19.0\n",
      "pandas 1.0.5\n",
      "\n",
      "Git hash: 1945bf82916f2d02283e663b3a8810ec7d103894\n",
      "Git repo: https://github.com/ysraell/aceleradev_private.git\n",
      "Git branch: master\n"
     ]
    }
   ],
   "source": [
    "# Run this cell before close.\n",
    "%watermark\n",
    "%watermark -p loguru\n",
    "%watermark -p scipy\n",
    "%watermark -p surprise\n",
    "%watermark --iversion\n",
    "%watermark -b -r -g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = '../data/'\n",
    "top_cols = pd.read_csv('top_cols.csv')['cols'].to_list()\n",
    "df_marked = pd.read_csv(path_data+'estaticos_market.csv', usecols=top_cols)\n",
    "col_user = 'id'\n",
    "top_cols.remove(col_user)\n",
    "\n",
    "rest_cols = []\n",
    "for col in top_cols:\n",
    "    df_marked[col] = df_marked[col].fillna(0)*1\n",
    "\n",
    "def normalize(x):\n",
    "    return (x-np.min(x))/(np.max(x) - np.min(x)) if (np.max(x) - np.min(x)) > 0 else (x-np.min(x))\n",
    "\n",
    "escala = 100\n",
    "for col in top_cols:\n",
    "    df_marked[col] = (escala*normalize(df_marked[col].tolist())).astype(np.uint8)\n",
    "    \n",
    "remove_cols = []\n",
    "for col in top_cols:\n",
    "    if df_marked[col].nunique() == 1:\n",
    "        remove_cols.append(col)\n",
    "\n",
    "df_marked = df_marked.drop(columns=remove_cols)\n",
    "for col in remove_cols:\n",
    "    top_cols.remove(col)\n",
    "\n",
    "df_marked = pd.melt(df_marked, id_vars=[\"id\"], var_name=\"itemID\", value_name=\"rating\").rename(columns={\"id\": \"userID\"})\n",
    "\n",
    "reader = Reader(rating_scale=(0, escala))\n",
    "#data = Dataset.load_from_df(df_marked[['userID', 'itemID', 'rating']].sample(frac=0.2), reader)\n",
    "data = Dataset.load_from_df(df_marked[['userID', 'itemID', 'rating']], reader)\n",
    "del df_marked\n",
    "\n",
    "df_ep_list = [pd.read_csv(path_data+'estaticos_portfolio{}.csv'.format(i+1)) for i in range(3)]\n",
    "tmp = []\n",
    "for i in range(3):\n",
    "    df_ep_list[i]['P'] = i+1 \n",
    "    tmp.append(df_ep_list[i][['id','P']])\n",
    "df_ep = pd.concat(tmp)\n",
    "del df_ep_list\n",
    "del tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExSVD(SVD):\n",
    "    \"\"\"\n",
    "        Classe extendida da surprise.SVD.\n",
    "        \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,**args):\n",
    "        self.matrix_dict = {}\n",
    "        super().__init__(**args)\n",
    "\n",
    "    def fit(self,trainset):\n",
    "        \"\"\"\n",
    "            Reimplementei a SVD.fit para colocar um logger nível INFO.\n",
    "        \"\"\"\n",
    "        logger.info(\"Treinando modelo SVD...\")\n",
    "        super().fit(trainset)\n",
    "        logger.info(\"Pronto!\")\n",
    "    \n",
    "    def get_neighbors(self,uid,k=1):\n",
    "        \"\"\"\n",
    "            Calcula todas as distâncias entre 'uid' de entrada e todos os outros 'uid'.\n",
    "            A distância calciulada é armazenda e não calculada novamente. \n",
    "        \"\"\"\n",
    "        logger.info(\"Calculando todos os vizinhos...\")\n",
    "        for uid2 in tqdm(self.trainset.all_users()):\n",
    "            ordered = tuple(sorted((uid,uid2)))\n",
    "            if (uid != uid2) and (ordered not in self.matrix_dict.keys()):\n",
    "                self.matrix_dict[ordered] = cosine(self.pu[uid],self.pu[uid2])\n",
    "        return [x[0] for x in sorted(\n",
    "            [\n",
    "                (uid2, self.matrix_dict[tuple(sorted((uid,uid2)))]) \n",
    "                for uid2 in self.trainset.all_users()\n",
    "                if uid != uid2\n",
    "            ], key=lambda x: x[1])][:k-1]\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use the famous SVD algorithm.\n",
    "ex_algo = ExSVD(n_factors=10, verbose=True)\n",
    "\n",
    "# Train the algorithm on the trainset, and predict ratings for the testset\n",
    "ex_algo.fit(data)\n",
    "\n",
    "\n",
    "# Then compute RMSE\n",
    "accuracy.rmse(predictions)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
