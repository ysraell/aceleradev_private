{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment D2 \n",
    "- Uma quebra no D para reimplemtação do SVD sem o módulo Surprise\n",
    "\n",
    "Ideas:\n",
    "\n",
    "1) Reimplementar o SVD.\n",
    "\n",
    "2) Implementar um framework de busca de hiperparâmetros.\n",
    "\n",
    "2.1) N fatores (`n_factors`) da decomposição FM.\n",
    "\n",
    "2.2) N top colunas (`top_cols`) do dataset.\n",
    "\n",
    "2.3) Parâmetro $L$ (`recomender(...,L,...)`).\n",
    "\n",
    "\n",
    "- Author: Israel Oliveira [\\[e-mail\\]](mailto:'Israel%20Oliveira%20'<prof.israel@gmail.com>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NewType, List\n",
    "import functools\n",
    "import operator\n",
    "from collections import Counter, defaultdict\n",
    "from copy import deepcopy\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from loguru import logger\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pythran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-27T13:17:02+00:00\n",
      "\n",
      "CPython 3.7.7\n",
      "IPython 7.15.0\n",
      "\n",
      "compiler   : GCC 8.3.0\n",
      "system     : Linux\n",
      "release    : 4.19.76-linuxkit\n",
      "machine    : x86_64\n",
      "processor  : \n",
      "CPU cores  : 16\n",
      "interpreter: 64bit\n",
      "loguru 0.5.1\n",
      "pythran 0.9.5\n",
      "pythran 0.9.5\n",
      "numpy   1.19.0\n",
      "pandas  1.0.5\n",
      "\n",
      "Git hash: e42c9fd2cd1afa9632ffef797dc170f68057ea0a\n",
      "Git repo: https://github.com/ysraell/aceleradev_private.git\n",
      "Git branch: master\n"
     ]
    }
   ],
   "source": [
    "# Run this cell before close.\n",
    "%watermark\n",
    "%watermark -p loguru\n",
    "%watermark -p pythran\n",
    "%watermark --iversion\n",
    "%watermark -b -r -g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From exp. C:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pythranized_d034442e4c6cd91b6dd545d08188a5d5.cpython-37m-x86_64-linux-gnu.so\n"
     ]
    }
   ],
   "source": [
    "!ls *.so && rm -f *.so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext pythran.magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pythran -fopenmp\n",
    "#pythran export normalizeitor(float64[][])\n",
    "#pythran export transformer(float64[][])\n",
    "#pythran export vector_distance_pythran(int8[][],int8[])\n",
    "#pythran export pairwise_distance_pythran(int8[][])\n",
    "\n",
    "def normalizeitor(x):\n",
    "    return (x -x.min())/(x.max() - x.min() +1e-10)\n",
    "\n",
    "def transformer(U):\n",
    "    for i in range(len(U)):\n",
    "        U[i] = 127*normalizeitor(U[i])\n",
    "    return U\n",
    "\n",
    "def vector_distance_pythran(X,vec):\n",
    "    return abs(X - vec).sum(-1)\n",
    "\n",
    "def pairwise_distance_pythran(X):\n",
    "    return abs(X[:, None, :] - X).sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pythranized_d034442e4c6cd91b6dd545d08188a5d5.cpython-37m-x86_64-linux-gnu.so\n"
     ]
    }
   ],
   "source": [
    "!ls *.so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat(a):\n",
    "    return functools.reduce(operator.iconcat, a, []) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-27 13:22:14.258 | INFO     | __main__:<module>:1 - Carregando e processando o dataset...\n",
      "2020-06-27 13:22:35.036 | INFO     | __main__:<module>:43 - ...pronto!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "logger.info(\"Carregando e processando o dataset...\")\n",
    "\n",
    "path_data = '../data/'\n",
    "top_cols = pd.read_csv('top_cols.csv')['cols'].to_list()\n",
    "df_marked = pd.read_csv(path_data+'estaticos_market.csv', usecols=top_cols)\n",
    "col_user = 'id'\n",
    "top_cols.remove(col_user)\n",
    "\n",
    "rest_cols = []\n",
    "for col in top_cols:\n",
    "    df_marked[col] = df_marked[col].fillna(0)*1\n",
    "    \n",
    "def normalize(x):\n",
    "    return (x-np.min(x))/(np.max(x) - np.min(x)) if (np.max(x) - np.min(x)) > 0 else (x-np.min(x))\n",
    "\n",
    "escala = 255\n",
    "for col in top_cols:\n",
    "    try:\n",
    "        df_marked[col] = (escala*normalize(df_marked[col].tolist())).astype(np.uint8)\n",
    "    except:\n",
    "        maping = {val:i+1 for i,val in enumerate(df_marked[col].unique())}\n",
    "        df_marked[col] = df_marked[col].apply(lambda x: maping[x])\n",
    "        df_marked[col] = (escala*normalize(df_marked[col].tolist())).astype(np.uint8)\n",
    "    \n",
    "remove_cols = []\n",
    "for col in top_cols:\n",
    "    if df_marked[col].nunique() == 1:\n",
    "        remove_cols.append(col)\n",
    "\n",
    "df_marked = df_marked.drop(columns=remove_cols)\n",
    "for col in remove_cols:\n",
    "    top_cols.remove(col)\n",
    "\n",
    "df_ep_list = [pd.read_csv(path_data+'estaticos_portfolio{}.csv'.format(i+1)) for i in range(3)]\n",
    "tmp = []\n",
    "for i in range(3):\n",
    "    df_ep_list[i]['P'] = i+1 \n",
    "    tmp.append(df_ep_list[i][['id','P']])\n",
    "df_ep = pd.concat(tmp)\n",
    "del df_ep_list\n",
    "del tmp\n",
    "\n",
    "logger.info(\"...pronto!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_marked.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Uid = NewType('uid', int)\n",
    "Raw = NewType('raw', str)\n",
    "\n",
    "class ExSVD():\n",
    "    \"\"\"\n",
    "        Classe para SVD.\n",
    "        \n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,stateless: bool = False):\n",
    "        self.matrix_dict = {}\n",
    "        self.matrix_dict_2 = {}\n",
    "        self.stateless = stateless\n",
    "\n",
    "    def fit(self,trainset = None):\n",
    "        \"\"\"\n",
    "            Reimplementei a SVD.fit para colocar um logger nível INFO.\n",
    "        \"\"\"\n",
    "        #logger.info(\"Treinando modelo SVD...\")\n",
    "        #logger.info(\"Pronto!\")\n",
    "    \n",
    "    def _get_neighbors(self,uid: Uid, k: int = 1, black_list: List[Uid] = []) -> List[Uid]:\n",
    "        \"\"\"\n",
    "            Calcula todas as distâncias entre 'uid' de entrada e todos os outros 'uid'.\n",
    "            A distância calciulada é armazenda e não calculada novamente. \n",
    "        \"\"\"\n",
    "        black_list.append(uid)\n",
    "        k = k if k >= 0 else 0\n",
    "        #logger.info(\"Calculando todos os vizinhos...\")\n",
    "        #for uid2 in tqdm(self.trainset.all_users()):\n",
    "        for uid2 in self.trainset.all_users():\n",
    "            ordered = tuple(sorted((uid,uid2)))\n",
    "            if (uid2 not in black_list) and (ordered not in self.matrix_dict.keys()):\n",
    "                self.matrix_dict[ordered] = cosine(self.pu[uid],self.pu[uid2])\n",
    "        out = [x[0] for x in sorted(\n",
    "            [\n",
    "                (uid2, self.matrix_dict[tuple(sorted((uid,uid2)))]) \n",
    "                for uid2 in self.trainset.all_users()\n",
    "                if (uid2 not in black_list)\n",
    "            ], key=lambda x: x[1])][:k-1]\n",
    "        if self.stateless:\n",
    "            del self.matrix_dict\n",
    "            self.matrix_dict = {}\n",
    "        return out\n",
    "\n",
    "    def _get_neighbors_2(self,uid: Uid, k: int = 1, black_list: List[Uid] = []) -> List[Uid]:\n",
    "        \"\"\"\n",
    "            Calcula todas as distâncias entre 'uid' de entrada e todos os outros 'uid'.\n",
    "            A distância calciulada é armazenda e não calculada novamente. \n",
    "        \"\"\"\n",
    "        black_list.append(uid)\n",
    "        k = k if k >= 0 else 0\n",
    "        #logger.info(\"Calculando todos os vizinhos...\")\n",
    "        #for uid2 in tqdm(self.trainset.all_users()):\n",
    "        if uid not in self.matrix_dict_2.keys():\n",
    "            Un = transformer(self.pu).astype(np.int8)\n",
    "            self.matrix_dict_2[uid] = vector_distance_pythran(Un,Un[uid])\n",
    "        out = [x[0] for x in sorted(\n",
    "            [\n",
    "                (uid2, self.matrix_dict_2[uid][uid2])\n",
    "                for uid2 in self.trainset.all_users()\n",
    "                if (uid2 not in black_list)\n",
    "            ], key=lambda x: x[1])][:k-1]\n",
    "        if self.stateless:\n",
    "            del self.matrix_dict\n",
    "            self.matrix_dict = {}\n",
    "        return out\n",
    "    \n",
    "    def _uid2raw(self, uid: Uid)-> str:\n",
    "        '''\n",
    "            uid -> raw.\n",
    "            Valor interno para externo, o nome original do usuário.\n",
    "        '''\n",
    "        return self.trainset.to_raw_uid(uid)\n",
    "    \n",
    "    def _raw2uid(self, raw: Raw)-> int:\n",
    "        '''\n",
    "            raw -> uid.\n",
    "            Valor externo para interno, o id interno do usuários..\n",
    "        '''\n",
    "        return self.trainset.to_inner_uid(raw)\n",
    "    \n",
    "    def recomender(self, in_list: List[Raw], k: int = 1, L: int = 3, Fk: int = 1, limit: int = 100)-> List[Raw]:\n",
    "        '''\n",
    "            Faz as recomendacoes.\n",
    "            ##### Função incompleta #####\n",
    "        '''\n",
    "        # Pega quantas recomendações por usuário em `in_list`,\n",
    "        # mas sem deixar faltar\n",
    "        N_in = len(in_list)\n",
    "        k = k if k > 0 else 1\n",
    "        R_per_in = L*(k//N_in + min(k%N_in,1))\n",
    "\n",
    "        # Pega os `uid`\n",
    "        uid_in_list = []\n",
    "        for raw in in_list:\n",
    "            uid_in_list.append(self._raw2uid(raw))\n",
    "\n",
    "        # Pega os vizinhos mais próximos de cada uid de entrada.\n",
    "        done = False\n",
    "        flag = True\n",
    "        Rounds = 0\n",
    "        while limit and (not done):\n",
    "            Rounds += 1\n",
    "            # Ele sempre pega todos novamente.\n",
    "            recomendations_list = []\n",
    "            for i,uid in enumerate(uid_in_list):\n",
    "                #logger.info(\"Calculando todos os vizinhos...{:,}/{:,} (Round: {:,}).\".format(i+1,N_in,Rounds))\n",
    "                #recomendations_list.append(self._get_neighbors(uid,R_per_in,in_list))\n",
    "                recomendations_list.append(self._get_neighbors_2(uid,R_per_in,in_list))\n",
    "            # Quando limit = 0, encerra.\n",
    "            limit -= 1\n",
    "            # Quando tem gente o suficiente, encerra.\n",
    "            if len(set(flat(recomendations_list))) >= Fk*k:\n",
    "                done = True\n",
    "            # Depois do primeiro loop, pega um a mais.\n",
    "            R_per_in += 1\n",
    "\n",
    "        # Aqui gera um dicionário ordenando por votacao.\n",
    "        count_rec = Counter(flat(recomendations_list)) # A votação!!\n",
    "        count_rec = list(count_rec.items())\n",
    "        ct_pos = defaultdict(list)\n",
    "        #ct_pos_inv = defaultdict(list)\n",
    "        while count_rec:\n",
    "            tmp = count_rec.pop(0)\n",
    "            ct_pos[tmp[1]].append(tmp[0])\n",
    "            #ct_pos_inv[tmp[0]].append(tmp[1])\n",
    "\n",
    "        # Aqui considera a posiçao de vizinhos mais proximos.\n",
    "        #nn_pos = defaultdict(list)\n",
    "        nn_pos_inv = defaultdict(list)\n",
    "        tmp = deepcopy(recomendations_list)\n",
    "        while tmp:\n",
    "            tmp2 = tmp.pop(0)\n",
    "            n = 0\n",
    "            while tmp2:\n",
    "                n += 1\n",
    "                tmp3 = tmp2.pop(0)\n",
    "                #nn_pos[n].append(tmp3)\n",
    "                nn_pos_inv[tmp3].append(n)\n",
    "\n",
    "        # Vai separando por votação e ordem de proximidade como desempate.      \n",
    "        votos_list = list(ct_pos.keys())\n",
    "        out_uid = []\n",
    "        while votos_list and k:\n",
    "            votos = max(votos_list)\n",
    "            votos_list.remove(votos)\n",
    "            tmp = sorted([(tmp, min(nn_pos_inv[tmp])) for tmp in ct_pos[votos]], key=lambda x: x[1])\n",
    "            while tmp and k:\n",
    "                out_uid.append(tmp.pop(0)[0])\n",
    "                k -= 1\n",
    "\n",
    "        # converte para Raw e \"joga fora\".\n",
    "        return [self._uid2raw(uid) for uid in out_uid]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing epoch 0\n",
      "Processing epoch 1\n",
      "Processing epoch 2\n",
      "Processing epoch 3\n",
      "Processing epoch 4\n",
      "Processing epoch 5\n",
      "Processing epoch 6\n",
      "Processing epoch 7\n",
      "Processing epoch 8\n",
      "Processing epoch 9\n",
      "Processing epoch 10\n",
      "Processing epoch 11\n",
      "Processing epoch 12\n",
      "Processing epoch 13\n",
      "Processing epoch 14\n",
      "Processing epoch 15\n",
      "Processing epoch 16\n",
      "Processing epoch 17\n",
      "Processing epoch 18\n",
      "Processing epoch 19\n"
     ]
    }
   ],
   "source": [
    "ex_algo = ExSVD(stateless=True, n_factors=200, n_epochs=20, verbose=True)\n",
    "ex_algo.fit(data.build_full_trainset())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passo de validação simples, para cada uma empresa no portfólio pegar N recomendações e ver se uma delas está no portfólio. Se está, soma 1, se não, soma 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empresa 10/10.\r"
     ]
    }
   ],
   "source": [
    "N = 10\n",
    "tmp = []\n",
    "n = 0\n",
    "Nu = 10\n",
    "times = []\n",
    "for row in df_ep.sample(n=Nu).iterrows():\n",
    "    t = time()\n",
    "    n += 1\n",
    "    print(\"Empresa {:,}/{:,}.\".format(n,Nu), end='\\r')\n",
    "    recs = ex_algo.recomender([row[1].id],k=N)\n",
    "    tmp.append(any([x in df_ep.loc[df_ep.P == row[1].P].id.to_list() for x in recs])*1)\n",
    "    times.append(time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.791889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.023075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.759529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.781416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.790698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.802646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.835960</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            time\n",
       "count  10.000000\n",
       "mean    0.791889\n",
       "std     0.023075\n",
       "min     0.759529\n",
       "25%     0.781416\n",
       "50%     0.790698\n",
       "75%     0.802646\n",
       "max     0.835960"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(times, columns=['time']).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Item 2 pronto**, usei o Pythran e ficou bem mais rápido, mas o passo de treino não está.\n",
    "Creio que é chegado o momento de jogar fora o Surprise antes de implementar a busca por hiperparâmetros. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
